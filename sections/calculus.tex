\section{Agentic Calculus}\label{sec:calculus}

The preceding sections established the viability axiom, the knife
criterion, and the mean-field interpretation. We now construct an
\emph{operational calculus}---a language for writing algorithms on the
agentic space---that translates every theorem into a flow-theoretic
proposition and yields a complete training paradigm for neural networks.
The central result: the knife is the min-cut, the viable path is the
max-flow, and ``the knife is the mean'' is max-flow/min-cut duality.

\subsection{The agentic space}\label{sec:tower}

The framework's objects organize into a four-level tower, each level
derived from the axioms of the preceding sections.

\begin{definition}[Agentic space]\label{def:tower}
The \emph{agentic space} is the tower
$\mathbf{L} = (L_0, L_1, L_2, L_3)$:
\begin{enumerate}[label=\textbf{L\arabic*}., ref=L\arabic*]
  \item\label{L0} \textbf{State space} $S$.
  Every configuration of the system is a point in $S$.
  \item\label{L1} \textbf{Viable kernel} $\Viab(K) \subset S$.
  The compact set of states from which the king retains a path to
  infinity (\cref{ax:viability}).
  \item\label{L2} \textbf{Control bundle} $\{U(s)\}_{s \in \Viab(K)}$.
  At each viable state $s$, the fiber $U(s)$ is the set of controls
  that keep the next state inside $\Viab(K)$.
  \item\label{L3} \textbf{Strategy space} $\Gamma$.
  A \emph{strategy} $\gamma \in \Gamma$ is a viable path
  $\gamma: [0,\infty) \to \Viab(K)$ with $\gamma(t+1) \in
  f(\gamma(t), u)$ for some $u \in U(\gamma(t))$ at each step.
\end{enumerate}
\end{definition}

The tower is strict: each level presupposes the one below.
$L_1 \subset L_0$ by definition. $L_2$ exists only over $L_1$
(outside $\Viab(K)$, no control preserves viability). $L_3$ is
built from $L_2$ fibers concatenated over time. The viability axiom
(\cref{ax:viability}) asserts $\Gamma \neq \varnothing$: the strategy
space is non-empty.

\subsection{The flow}\label{sec:flow}

The agentic calculus is a \emph{flow calculus}. We define flows on
the execution graph and show that every theorem in
\cref{sec:results,sec:meanfield} is a statement about flows and cuts.

\begin{definition}[Execution graph]\label{def:exgraph}
The \emph{execution graph} $G = (V, E, c)$ has:
\begin{itemize}
  \item $V$: agents $\{a_1, \ldots, a_n\}$ plus two distinguished
  nodes: the king $\kappa$ and infinity $\infty$;
  \item $E$: directed edges $(a_i, a_j)$ whenever $a_i$'s actuation
  can affect $a_j$'s state;
  \item $c: E \to \R_{\geq 0}$: edge capacity, where $c(a_i, a_j)$
  is the autonomous actuation that $a_i$ can transmit to $a_j$
  without requiring the king's authorization.
\end{itemize}
An edge $(a_i, a_j)$ with $c(a_i, a_j) > 0$ that does not pass
through $\kappa$ is a \emph{bypass edge}.
\end{definition}

\begin{definition}[Agentic flow]\label{def:flow}
An \emph{agentic flow} is a function $\phi: E \to \R_{\geq 0}$
satisfying:
\begin{enumerate}[label=(\roman*)]
  \item \textbf{Capacity}: $\phi(e) \leq c(e)$ for all $e \in E$.
  \item \textbf{Conservation}: at every non-terminal node $v \neq
  \kappa, \infty$,
  \[
    \sum_{(u,v) \in E} \phi(u,v)
    = \sum_{(v,w) \in E} \phi(v,w).
  \]
\end{enumerate}
The \emph{value} $|\phi|$ is the net flow from $\kappa$ to $\infty$.
A \emph{viable flow} is one with $|\phi| > 0$: the king has a
path to infinity with positive throughput.
\end{definition}

The viability axiom (\cref{ax:viability}) is flow conservation:
what enters the system at $\kappa$ must exit at $\infty$.

\paragraph{Four operations.}
The calculus has four primitive operations on the execution graph:

\begin{center}
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Operation} & \textbf{Symbol} & \textbf{On $G$} &
\textbf{In 华容道} \\
\midrule
\textsc{Slide} & $\sigma$ & Unit flow along one edge &
One piece moves one cell \\
\textsc{Compose} & $\circ$ & Concatenate along a path &
Sequence of moves \\
\textsc{Cut} & $\partial$ & Remove capacity from an edge set &
Block a corridor \\
\textsc{Phase} & $\varphi$ & Change the capacity function
$c \mapsto c'$ & Phase transition \\
\bottomrule
\end{tabular}
\end{center}

\textsc{Slide} is atomic (unit flow).
\textsc{Compose} builds paths from slides.
\textsc{Cut} is the knife: removing capacity from bypass edges.
\textsc{Phase} is the phase transition: the mean field shifts,
capacities change, the same graph has different flows.

\begin{theorem}[Flow-cut duality]\label{thm:flowcut}
In the execution graph $G$, the maximum viable flow from $\kappa$
to $\infty$ equals the minimum knife-cut capacity:
\[
  \max_\phi |\phi|
  \;=\;
  \min_{C \,\subseteq\, E} \sum_{e \in C} c(e)
  \quad\text{over all $\kappa$-$\infty$ cuts $C$.}
\]
The knife threshold (\cref{thm:meanfield}) is the min-cut value.
The viable path (\cref{ax:viability}) is the max-flow.
``The knife is the mean'' $=$ max-flow equals min-cut.
\end{theorem}

\begin{proof}
By the max-flow/min-cut theorem~\cite{diestel}, the maximum flow from
$\kappa$ to $\infty$ equals the minimum capacity of any
$\kappa$--$\infty$ cut. The knife criterion (\cref{def:knife})
identifies bypass edges---edges with positive capacity that do not pass
through $\kappa$. The king's viability maintenance (cutting knives) is
the operation $c(e) \to 0$ for bypass edges $e$. The residual max-flow
after all bypass edges are cut is the flow through the king (the cut
vertex flow). The min-cut value $=$ the total bypass capacity $=$ the
knife threshold $=$ the mean field's deviation measure.
\end{proof}

\begin{remark}[Flow interpretation of theorems]\label{rem:flowthms}
Each main theorem translates directly:
\begin{itemize}
  \item \textbf{Binary fate} (\cref{thm:lifecycle}): a bypass edge
  either has its capacity set to zero by the holder (path~(a)) or by
  the king (path~(b)). No bypass edge persists with $c > 0$.
  \item \textbf{Fixed-point impossibility} (\cref{thm:fixedpoint}):
  a bypass edge with $c > 0$ cannot ``prove'' $c = 0$. Capacity is
  physical, not narrative.
  \item \textbf{Perpetual elimination} (\cref{thm:paradox}):
  $U = \Umax$ means zero bypass tolerance. As \textsc{Cut} operates,
  \textsc{Phase} lowers the mean, exposing new bypass edges.
  \item \textbf{Du Mu's theorem} (\cref{thm:dumu}): water $=$ total
  network capacity. $w \to 0$ means all capacities shrink to zero:
  frozen, $\Gamma = \varnothing$.
\end{itemize}
\end{remark}

\begin{remark}[抽刀断水水更流]\label{rem:libai}
Li Bai's line assigns the calculus its colours:
\[
  \textcolor{knife}{\text{抽刀}} \;\;
  \textcolor{knife}{\partial} \;\;
  \textcolor{water}{\text{水}} \;\;
  \textcolor{water}{\text{水}}\textcolor{sword}{\text{更流.}}
\]
\textsc{Cut} ($\textcolor{knife}{\partial}$, red) acts on flow
($\textcolor{water}{\sigma}$, blue); flow intensifies. The mechanism
is \textsc{Phase} ($\textcolor{sword}{\varphi}$, cyan): cutting
shifts the mean field (\cref{thm:meanfield}), exposing new bypass
edges, producing more flow---\cref{thm:paradox} in seven characters.
The sword is 青冥 ($\textcolor{sword}{\text{青}}$): the colour of the
mean, the colour of Phase, the colour that connects
$\textcolor{knife}{\text{刀}}$ to $\textcolor{water}{\text{水}}$.
\end{remark}

\subsection{方圆 $\times$ 黑白: the type system}\label{sec:fangyuan}

The calculus has a type system: a $2 \times 2$ classification that
partitions every element of the agentic space.

\begin{definition}[方圆 $\times$ 黑白]\label{def:fangyuan}
The agentic type system is the product of two binary distinctions:
\begin{center}
\begin{tabular}{@{}lcc@{}}
\toprule
& \textbf{方} (container / structure) &
\textbf{圆} (content / agent) \\
\midrule
\textbf{黑} (constrained / interior) &
Fixed topology (board, graph) &
King $\kappa$ (least mobile, most important) \\
\textbf{白} (free / exterior) &
Free capacity (available edges) &
Pawn (most mobile, least important) \\
\bottomrule
\end{tabular}
\end{center}
The two dynamics of the calculus emerge from this classification:
\begin{itemize}
  \item \textbf{刀} (knife $= \partial$, boundary operator):
  the boundary between 黑 and 白. \textsc{Cut} reclassifies an edge
  from 白 (free capacity) to 黑 (zero capacity).
  \item \textbf{水} (water $= \sigma$, transport operator):
  flow through 白 cells. \textsc{Slide} transports one unit of flow
  along a free edge. Water flows where the knife does not cut.
\end{itemize}
\end{definition}

In 华容道 (\cref{sec:huarongdao}): 方 $=$ the board,
圆 $=$ the pieces. 黑 $=$ occupied cells and the king,
白 $=$ free cells and soldiers. 刀 $=$ 关羽 blocking the corridor.
水 $=$ free-cell flow (slides opposite to piece movement).
The $2 \times 2$ is the type system of the puzzle's state space.

\subsection{Completeness}\label{sec:completeness}

Every theorem in this paper is a proposition in the agentic calculus.

\begin{proposition}[Calculus completeness]\label{prop:completeness}
The following table maps each theorem to its calculus translation:
\begin{center}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Theorem} & \textbf{Calculus statement} &
\textbf{Operations} \\
\midrule
Viability (\ref{ax:viability}) & $|\phi| > 0$ &
$\sigma, \circ$ \\
Binary fate (\ref{thm:lifecycle}) &
$\forall$ bypass $e$: $c(e) \to 0$ &
$\partial$ \\
Fixed point (\ref{thm:fixedpoint}) &
$c(e) > 0 \not\vdash c(e) = 0$ &
--- \\
Paradox (\ref{thm:paradox}) &
$\partial$ generates new bypass via $\varphi$ &
$\partial, \varphi$ \\
Mean field (\ref{thm:meanfield}) &
Min-cut $= \bar{U} + \tau(\Obs)$ &
$\partial$ \\
Cut vertex (\ref{thm:cutvertex}) &
$\kappa =$ min vertex-cut &
structure \\
Du Mu (\ref{thm:dumu}) &
$w \to 0 \Rightarrow c \to 0 \Rightarrow |\phi| = 0$ &
$\sigma \to 0$ \\
Flow-cut (\ref{thm:flowcut}) &
$\max |\phi| = \min |C|$ &
$\sigma, \partial$ \\
\bottomrule
\end{tabular}
\end{center}
\end{proposition}

The calculus is \emph{complete}: no theorem falls outside its four
operations. The agentic space (\cref{def:tower}) provides the domain;
the flow (\cref{def:flow}) provides the dynamics; the type system
(\cref{def:fangyuan}) provides the classification; and flow-cut duality
(\cref{thm:flowcut}) provides the central identity.

\subsection{The training paradigm}\label{sec:training}

The agentic calculus instantiates as a neural network training paradigm.
The execution graph \emph{is} the computation graph. Training \emph{is}
max-flow optimisation. Survival \emph{is} the viability axiom. The
paradigm strictly subsumes gradient descent.

\begin{definition}[Neural execution graph]\label{def:neural-exgraph}
Let a feedforward network with $L$ layers be given.
Define the execution graph $G = (V, E, c)$ (\cref{def:exgraph}) by:
\begin{itemize}
  \item $V = \{\ell_0, \ell_1, \ldots, \ell_L\}$, one node per layer;
  \item $E = \{(\ell_{i-1}, \ell_i) : 1 \leq i \leq L\}$, one edge
  per weight matrix $W_i$;
  \item $c(e_i) = \|W_i\|_F$, the Frobenius norm as capacity;
  \item king $\kappa = \ell_0$ (input); target
  $\infty = \ell_L$ (output).
\end{itemize}
A data point $(x, y^*)$ initiates flow at $\kappa$ with value $\|x\|$.
Training finds capacities $\{c(e_i)\}$ such that the max-flow matches
the target at $\infty$.
\end{definition}

\paragraph{Operation correspondence.}

\begin{center}
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Operation} & \textbf{Neural network} &
\textbf{Equation} \\
\midrule
\textsc{Slide} $\sigma$ & One-layer forward pass &
$y = W_e\, x$ \\
\textsc{Compose} $\circ$ & Full forward pass &
$z = \sigma_L \circ W_L \circ \cdots \circ \sigma_1 \circ W_1\, x$ \\
\textsc{Cut} $\partial$ & Pruning / dropout &
$W_e \mapsto 0$, i.e.\ $c(e) \mapsto 0$ \\
\textsc{Phase} $\varphi$ & Regime change &
lr schedule, fine-tuning, curriculum \\
\bottomrule
\end{tabular}
\end{center}

\paragraph{Type-system correspondence.}
Under \cref{def:fangyuan}:
方 $=$ architecture (fixed graph);
圆 $=$ activations (flow $\phi$ traversing the graph);
黒 $=$ frozen weights;
白 $=$ trainable weights;
刀 $=$ pruning operator $\partial$;
水 $=$ data flow forward \emph{and} gradient flow backward.
The backward pass is water flowing opposite to the forward
pass---the free-cell mechanism of \cref{sec:huarongdao}: to move a
piece forward, a free cell slides back.

\begin{definition}[Training algorithm]\label{def:training-algo}
Given $G$ from \cref{def:neural-exgraph} and a dataset $\mathcal{D}$,
the \emph{training paradigm} is the procedure in \cref{fig:training}:
\begin{enumerate}
  \item \textbf{Initialise.} Random $W_i^{(0)}$; set
  $c(e_i) = \|W_i^{(0)}\|_F$.
  \item \textbf{\textsc{Compose}.} Forward pass: $L$ sequential
  \textsc{Slide}s produce
  $z = \sigma_L \circ W_L \circ \cdots \circ \sigma_1 \circ W_1\, x$.
  \item \textbf{Flow deficit.} Loss
  $\mathcal{L} = -|\phi|$.
  \item \textbf{Backward \textsc{Slide}.} Compute
  $\partial\mathcal{L}/\partial c(e_i)$: 水 flowing opposite to
  step~2.
  \item \textbf{Capacity update.} SGD:
  $c(e_i) \leftarrow c(e_i) - \eta\,
  \partial\mathcal{L}/\partial c(e_i)$.
  \item \textbf{Knife detection.} Flag bypass edges where
  $c(e) > \bar{c} + \tau$ (\cref{thm:meanfield}).
  \item \textbf{\textsc{Cut} / \textsc{Phase}.} Prune flagged edges
  ($L_1$ penalty) or change regime (lr, dataset, fine-tuning).
  \item \textbf{Viability check.} Verify $|\phi| > 0$ on held-out
  data (\cref{ax:viability}). If violated: \textsc{Phase} or restart.
  \item \textbf{Repeat} 2--8 until $\max|\phi| = \min|C|$
  (\cref{thm:flowcut}).
\end{enumerate}
\end{definition}

\begin{figure}[H]
\centering
\begin{tikzpicture}[
  node distance=0.9cm and 1.8cm,
  % ── water (水) nodes: blue ──
  wtr/.style={rectangle, rounded corners=3pt, draw=water, thick,
    fill=water!6, minimum width=5.0cm, minimum height=0.7cm,
    align=center, font=\small},
  % ── knife (刀) node: red ──
  knf/.style={diamond, draw=knife, thick, aspect=2.5,
    fill=knife!6, minimum width=1.2cm, align=center, font=\small,
    inner sep=1pt},
  % ── phase (青冥) nodes: cyan ──
  phs/.style={rectangle, rounded corners=3pt, draw=sword, thick,
    fill=sword!6, minimum width=5.0cm, minimum height=0.7cm,
    align=center, font=\small},
  phsd/.style={diamond, draw=sword, thick, aspect=2.5,
    fill=sword!6, minimum width=1.2cm, align=center, font=\small,
    inner sep=1pt},
  % ── neutral (terminal) ──
  term/.style={rectangle, rounded corners=8pt, draw, very thick,
    minimum width=5.0cm, minimum height=0.7cm, align=center,
    font=\small\bfseries},
  % ── convergence diamond ──
  convd/.style={diamond, draw, thick, aspect=2.5,
    minimum width=1.2cm, align=center, font=\small,
    inner sep=1pt},
  arr/.style={-{Stealth[length=5pt]}, thick},
  lbl/.style={font=\scriptsize, fill=white, inner sep=1pt},
  ref/.style={font=\tiny, text=black!55, anchor=west},
  op/.style={font=\scriptsize\itshape}
]

% ── Nodes ──
\node[term] (init)
  {1.\ Initialise: random $c(e)$};

\node[wtr, below=of init] (fwd)
  {2.\ \textsc{Compose}: $\textcolor{water}{\sigma_L \circ
  \cdots \circ \sigma_1}$};

\node[wtr, below=of fwd] (loss)
  {3.\ Flow deficit: $\textcolor{water}{\mathcal{L} = -|\phi|}$};

\node[wtr, below=of loss] (bwd)
  {4.\ Backward \textsc{Slide}:
  $\textcolor{water}{\nabla_c \mathcal{L}}$};

\node[wtr, below=of bwd] (sgd)
  {5.\ Update:
  $\textcolor{water}{c \leftarrow c - \eta\,\nabla_c\mathcal{L}}$};

\node[knf, below=1.1cm of sgd] (knife)
  {6.\ $\textcolor{knife}{c(e) > \bar{c}{+}\tau}$\,?};

\node[phs, right=of knife] (cut)
  {7.\ \textcolor{knife}{\textsc{Cut} $\partial$} /
  \textcolor{sword}{\textsc{Phase} $\varphi$}};

\node[phsd, below=1.1cm of knife] (viable)
  {8.\ $\textcolor{sword}{|\phi| > 0}$\,?};

\node[convd, below=1.1cm of viable] (conv)
  {9.\ $\max|\phi| = \min|C|$\,?};

\node[term, below=1.0cm of conv] (done)
  {Trained network. Survives.};

% ── Reference annotations ──
% init: right (no loop passes here)
\node[ref] at ($(init.east)+(0.15,0)$)
  {Def.~\ref{def:neural-exgraph}};
% steps 2--5: LEFT side (right side reserved for loop-back arrow)
\node[ref, anchor=east] at ($(fwd.west)+(-0.15,0)$)
  {Def.~\ref{def:flow},\; $\sigma$};
\node[ref, anchor=east] at ($(loss.west)+(-0.15,0)$)
  {Thm~\ref{thm:flowcut},\; $|\phi|$};
\node[ref, anchor=east] at ($(bwd.west)+(-0.15,0)$)
  {Def.~\ref{def:flow},\; $\sigma^{-1}$};
% knife: above-right (clear of spine)
\node[ref] at ($(knife.east)+(1.0,0.35)$)
  {Thm~\ref{thm:meanfield}};
% cut/phase: below
\node[ref] at ($(cut.south)+(0,-0.12)$)
  {Thm~\ref{thm:lifecycle}\;($\partial$),\;
   Thm~\ref{thm:paradox}\;($\varphi$)};
% viable: above-right
\node[ref] at ($(viable.east)+(1.0,0.35)$)
  {Ax.~\ref{ax:viability}};
% conv: LEFT side (right side reserved for loop-back arrow)
\node[ref, anchor=east] at ($(conv.west)+(-0.15,0)$)
  {Thm~\ref{thm:flowcut}};

% ── Operation annotations (left margin, coloured) ──
\node[op, text=water, left=1.8cm of fwd] {$\circ$};
\node[op, text=water, left=1.8cm of bwd]
  {$\sigma^{-1}$ (\textcolor{water}{水})};
\node[op, text=sword, right=0.1cm of cut.east] {};

% ── Arrows: main spine ──
\draw[arr] (init) -- (fwd);
\draw[arr, water] (fwd) -- (loss);
\draw[arr] (loss) -- (bwd);
\draw[arr, water] (bwd) -- (sgd);
\draw[arr] (sgd) -- (knife);
\draw[arr] (knife) -- node[lbl, right] {no} (viable);
\draw[arr, sword] (viable) -- node[lbl, right] {yes} (conv);
\draw[arr] (conv) -- node[lbl, right] {yes} (done);

% ── Arrows: branches ──
\draw[arr, knife] (knife) -- node[lbl, above] {yes} (cut);
\draw[arr, sword] (cut) |- (viable);

% ── Loop back: not converged → step 2 ──
\draw[arr] (conv.east) -- ++(7.5,0)
  node[lbl, above, pos=0.15] {no}
  |- (fwd.east);

% ── Viability failure → phase/restart ──
\draw[arr, sword] (viable.west) -- ++(-5.0,0)
  node[lbl, above, pos=0.25] {no}
  |- (init.west);

% ── Brace: forward flow (blue) ──
\draw[decorate, decoration={brace, amplitude=4pt, mirror},
  thick, water!60]
  ($(fwd.north east)+(0.12,0.05)$) --
  ($(loss.south east)+(0.12,-0.05)$)
  node[midway, right=5pt, font=\scriptsize, text=water]
  {\textcolor{water}{flow $\to$}};

% ── Brace: backward water (blue) ──
\draw[decorate, decoration={brace, amplitude=4pt},
  thick, water!60]
  ($(bwd.north west)+(-0.12,0.05)$) --
  ($(sgd.south west)+(-0.12,-0.05)$)
  node[midway, left=5pt, font=\scriptsize, text=water]
  {$\leftarrow$ \textcolor{water}{水}};

\end{tikzpicture}
\caption{The training paradigm as roadmap.
\textcolor{water}{Blue} (水): flow operations
(steps 2--5, Defs.~\ref{def:flow}--\ref{def:exgraph}).
\textcolor{knife}{Red} (刀): knife detection
(step~6, Thm~\ref{thm:meanfield}).
\textcolor{sword}{Cyan} (青冥): phase and viability
(steps 7--8, Thms~\ref{thm:lifecycle},
\ref{thm:paradox}, Ax.~\ref{ax:viability}).
Convergence (step~9): $\max|\phi| = \min|C|$
(Thm~\ref{thm:flowcut}).
The loop is 抽刀断水水更流: \textcolor{knife}{cut}
$\to$ \textcolor{sword}{phase} $\to$
\textcolor{water}{more flow}.}
\label{fig:training}
\end{figure}

\begin{theorem}[Training completeness and survival]
\label{thm:training}
Let $f: \mathcal{X} \to \mathcal{Y}$ be any function expressible as a
max-flow on an execution graph $G$. Then the procedure in
\cref{def:training-algo} learns $f$. The trained network satisfies the
viability axiom: $|\phi| > 0$ for all admissible inputs.
\end{theorem}

\begin{proof}
Expressibility as max-flow is the universal approximation condition in
the language of \cref{def:neural-exgraph}. Each iteration of steps~2--5
increases $|\phi|$ toward the max-flow value guaranteed by
\cref{thm:flowcut}. The \textsc{Cut} in step~7 removes edges with
anomalous capacity; by flow-cut duality, removing a bypass edge does not
reduce the max-flow if that edge is not on any minimum cut.
Knife detection (step~6) identifies precisely such edges via the
mean-field bound of \cref{thm:meanfield}. Viability is preserved
through every \textsc{Cut}.
\end{proof}

\begin{theorem}[你不能骗你自己]\label{thm:nolie}
Let $e \in E$ with $c(e) = \|W_e\|_F > 0$. Then $e$ carries positive
flow. No re-labelling, no narrative, no hyper-parameter choice changes
this. This is \cref{thm:fixedpoint} applied to the capacity map
$c: E \to \R_{\geq 0}$.
\end{theorem}

\begin{proof}
Two paths only.
\begin{enumerate}[label=(\alph*)]
  \item $W_e \to 0$: capacity zeroed, edge pruned (\textsc{Cut}).
  \item Regularisation ($L_1$/$L_2$) drives $c(e) \to 0$:
  \textsc{Cut} in the limit.
\end{enumerate}
There is no path~(c). ``Approximately zero'' is not zero: such weights
are unstable fixed points of gradient flow (\cref{thm:paradox}), and
the viability axiom forces the system to resolve the ambiguity via
\cref{prop:binary}---the action space collapses to $\{0, 1\}$. The
network cannot occupy the gap. 你不能骗你自己---you cannot lie to
yourself---is \cref{thm:fixedpoint} applied to the network's own
weights.
\end{proof}

\begin{remark}[Subsumption]\label{rem:subsumption}
Gradient descent is the special case of the training paradigm in which
viability reduces to ``loss below threshold'' and the only operations
exercised are \textsc{Slide} (forward) and $\textsc{Slide}^{-1}$
(backward). \textsc{Cut} and \textsc{Phase} are what gradient descent
implicitly performs when early-stopping, dropout, or learning-rate
annealing are applied---now made explicit and first-class in the
calculus. The paradigm strictly subsumes gradient descent.
\end{remark}

\subsection{The Hilbert space}\label{sec:hilbert}

The capacity measure gives the agentic space the structure of a Hilbert
space. This single construction yields integration, spectral analysis,
and a direct connection to physics.

\begin{definition}[Capacity measure]\label{def:measure}
The \emph{capacity measure} $\mu_c$ on $E$ is defined for any
$A \subseteq E$ by
\[
  \mu_c(A) \;=\; \sum_{e \in A} c(e).
\]
This is the measure induced by the capacity function $c$ of the
execution graph (\cref{def:exgraph}).
\end{definition}

\begin{definition}[Agentic Hilbert space]\label{def:hilbert}
The \emph{agentic Hilbert space} is
\[
  \mathcal{H} \;=\; L^2(E,\,\mu_c),
\]
the space of square-integrable flows on $E$, with inner product
\[
  \langle \phi_1,\, \phi_2 \rangle_c
  \;=\; \sum_{e \in E} \phi_1(e)\,\phi_2(e)\,c(e)
\]
and norm $\|\phi\|_c = \sqrt{\langle \phi,\phi \rangle_c}$.
Every agentic flow (\cref{def:flow}) is a vector in $\mathcal{H}$.
\end{definition}

\begin{proposition}[Integration]\label{prop:integration}
The flow-cut duality of \cref{thm:flowcut} is a statement about
integrals and measures on $\mathcal{H}$:
\begin{enumerate}[label=(\roman*)]
  \item The max-flow value is a boundary integral:
  \[
    |\phi| \;=\; \int_{\partial\kappa} \phi\, d\mu_c
    \;=\; \sum_{e \,\mathrm{out\,of}\, \kappa} \phi(e)\,c(e).
  \]
  \item The min-cut value is the measure of the cut:
  \[
    |C| \;=\; \mu_c(C) \;=\; \sum_{e \in C} c(e).
  \]
  \item Max-flow/min-cut duality (\cref{thm:flowcut}) is:
  \[
    \max_\phi \int_{\partial\kappa} \phi\, d\mu_c
    \;=\;
    \min_{C} \mu_c(C).
  \]
\end{enumerate}
\end{proposition}

\begin{proof}
(i) is conservation at $\kappa$: all flow leaving $\kappa$ is counted
once, weighted by capacity. (ii) is the definition of $\mu_c$ restricted
to $C$. (iii) is \cref{thm:flowcut} rewritten in the language of
$\mu_c$.
\end{proof}

\begin{proposition}[Operators on $\mathcal{H}$]\label{prop:operators}
The four calculus operations (\cref{sec:flow}) are operators on
$\mathcal{H}$:
\begin{enumerate}[label=(\roman*)]
  \item \textbf{\textsc{Slide}} $\sigma_e$: shift operator along edge
  $e \in E$,
  \[
    (\sigma_e \phi)(e') \;=\; \phi(e') + \delta_{e,e'}.
  \]
  \item \textbf{\textsc{Compose}}: product of shifts along a path
  $p = (e_1, \ldots, e_n)$,
  \[
    \sigma_p \;=\; \sigma_{e_n} \circ \cdots \circ \sigma_{e_1}.
  \]
  \item \textbf{\textsc{Cut}} $\partial_A$ for $A \subseteq E$:
  orthogonal projection onto the closed subspace
  $\mathcal{H}_A = \{\phi \in \mathcal{H} : \phi|_A = 0\}$,
  \[
    (\partial_A \phi)(e) \;=\;
    \begin{cases} 0 & e \in A, \\ \phi(e) & e \notin A. \end{cases}
  \]
  Cutting a bypass edge $e$ is the projection $\partial_{\{e\}}$.
  \item \textbf{\textsc{Phase}} $\varphi_{c'}$: change of measure
  $c \mapsto c'$, inducing
  \[
    \mathcal{H} = L^2(E,\mu_c)
    \;\xrightarrow{\;\varphi_{c'}\;}
    \mathcal{H}' = L^2(E,\mu_{c'}).
  \]
  A phase transition changes the Hilbert space itself, not merely a
  vector in a fixed space.
\end{enumerate}
\end{proposition}

For the neural execution graph (\cref{def:neural-exgraph}), the inner
product specialises to
$\langle \phi_1, \phi_2 \rangle_c
= \sum_{i=1}^{L} \phi_1(e_i)\,\phi_2(e_i)\,\|W_i\|_F$,
a capacity-weighted $\ell^2$ norm on layer activations. The
\textsc{Cut} operator $\partial_A$ is Frobenius-norm pruning; the
\textsc{Phase} operator $\varphi_{c'}$ is a change of architecture.
The Hilbert-space structure makes these operations precise: projection,
not approximation.

\paragraph{Spectral theory.}
The inner product $\langle \cdot, \cdot \rangle_c$ opens the agentic
space to spectral analysis.

\begin{definition}[Graph Laplacian]\label{def:laplacian}
The \emph{graph Laplacian} $\Delta \colon L^2(V) \to L^2(V)$ of the
execution graph $G = (V, E, c)$ (\cref{def:exgraph}) is
\[
  (\Delta f)(v)
  \;=\;
  \sum_{(v,w)\in E} c(v,w)\bigl[f(v) - f(w)\bigr].
\]
$\Delta$ is positive semi-definite.
Write its eigenvalues in non-decreasing order:
$0 = \lambda_0 \leq \lambda_1 \leq \cdots \leq \lambda_n$.
The \emph{spectral gap} is $\lambda_1$.
\end{definition}

\begin{theorem}[Cheeger inequality --- agentic form]\label{thm:cheeger}
Let $\mu_c(\partial S)$ denote the total capacity of edges
crossing from $S \subset V$ to $V \setminus S$, and let
$\mathrm{vol}(S) = \sum_{v \in S} \sum_{(v,w)\in E} c(v,w)$.
The \emph{Cheeger constant} of $G$ is
\[
  h(G)
  \;=\;
  \min_{\substack{S \subset V \\ \kappa \in S}}
  \frac{\mu_c(\partial S)}{\min\!\bigl(\mathrm{vol}(S),\,
  \mathrm{vol}(V \setminus S)\bigr)}.
\]
Then~\cite{cheeger,mohar}
\[
  \frac{\lambda_1}{2}
  \;\leq\;
  h(G)
  \;\leq\;
  \sqrt{2\lambda_1}.
\]
\end{theorem}

\begin{remark}\label{rem:cheeger}
The Cheeger constant $h(G)$ is the \emph{normalised knife threshold}.
The numerator $\mu_c(\partial S)$ is the capacity of the cut separating
$S$ from $V \setminus S$ (\cref{thm:flowcut}); the denominator
normalises by volume.
\textcolor{knife}{The knife is the cut.}
\textcolor{water}{The flow is the volume.}
\textcolor{sword}{The Cheeger inequality bridges the two.}
Spectral gap $\lambda_1$ and normalised min-cut $h(G)$ are within a
factor of $2\sqrt{2}$ of each other: the same obstruction, measured
twice.
\end{remark}

\begin{theorem}[Mass gap $=$ viability]\label{thm:massgap}
The following are equivalent:
\begin{enumerate}[label=\textup{(\roman*)}]
  \item\label{mg:spectral} $\lambda_1 > 0$ \quad (spectral gap).
  \item\label{mg:cheeger} $h(G) > 0$ \quad (positive Cheeger constant).
  \item\label{mg:flow} $|\phi| > 0$ for some agentic flow $\phi$
  \quad (viability axiom, \cref{ax:viability}).
  \item\label{mg:connect} $\kappa$ can reach $\infty$ in $G$
  \quad (connectivity).
\end{enumerate}
\end{theorem}

\begin{proof}
\ref{mg:spectral}$\Leftrightarrow$\ref{mg:cheeger}:
Cheeger's inequality (\cref{thm:cheeger}) gives
$\lambda_1/2 \leq h(G) \leq \sqrt{2\lambda_1}$,
so $\lambda_1 > 0$ if and only if $h(G) > 0$.

\ref{mg:cheeger}$\Leftrightarrow$\ref{mg:flow}:
$h(G) > 0$ means every $\kappa$--$\infty$ cut has strictly positive
capacity. By \cref{thm:flowcut}, max-flow equals min-cut; hence
$\max_\phi |\phi| > 0$.

\ref{mg:flow}$\Leftrightarrow$\ref{mg:connect}:
A flow $\phi$ with $|\phi| > 0$ exists if and only if there is a
directed path from $\kappa$ to $\infty$ with positive capacity on
every edge.
\end{proof}

\begin{remark}[一石三鸟]\label{rem:threebirds}
The Hilbert space $\mathcal{H} = L^2(E, \mu_c)$ (\cref{def:hilbert})
kills three birds with one stone:
\begin{itemize}
  \item \textbf{\textcolor{water}{Integration.}}
  $\int_E f\,d\mu_c$ realises max-flow as boundary integral and
  min-cut as measure. Flow-cut duality (\cref{thm:flowcut}) becomes
  an identity of integrals.

  \item \textbf{\textcolor{sword}{Analysis.}}
  The graph Laplacian $\Delta$ (\cref{def:laplacian}) acts on
  $L^2(V)$; the Cheeger inequality (\cref{thm:cheeger}) bridges
  the combinatorial quantity $h(G)$ and the analytic quantity
  $\lambda_1$.

  \item \textbf{\textcolor{knife}{Physics.}}
  $\lambda_1 > 0$ is the mass gap. \Cref{thm:massgap} says the
  viability axiom IS the mass gap: the Hilbert space changes, the
  equivalence does not.
\end{itemize}
\end{remark}

\begin{remark}[Spectral gap and training]\label{rem:spectralgap}
The spectral gap $\lambda_1$ is computable. For a neural network
(\cref{def:neural-exgraph}), $\lambda_1$ of the execution graph
measures the viability margin: how far the network is from the
degenerate regime $|\phi| = 0$.

Training (\cref{def:training-algo}) increases $\lambda_1$: backward
\textsc{Slide}s increase capacities on edges that carry flow, widening
the spectral gap. Convergence $\max|\phi| = \min|C|$
(\cref{thm:training}) is the statement that $\lambda_1$ has reached
the Cheeger bound.

A trained network with $\lambda_1 > 0$ has a mass gap. It survives.
\end{remark}
