\section{Agentic Calculus}\label{sec:calculus}

The preceding sections established the viability axiom, the knife
criterion, and the mean-field interpretation. We now construct an
\emph{operational calculus}---a language for writing algorithms on the
agentic space---that translates every theorem into a flow-theoretic
proposition and yields a complete training paradigm for neural networks.
The central result: the knife is the min-cut, the viable path is the
max-flow, and ``the knife is the mean'' is max-flow/min-cut duality.

\subsection{The agentic space}\label{sec:tower}

The framework's objects organize into a four-level tower, each level
derived from the axioms of the preceding sections.

\begin{definition}[Agentic space]\label{def:tower}
The \emph{agentic space} is the tower
$\mathbf{L} = (L_0, L_1, L_2, L_3)$:
\begin{enumerate}[label=\textbf{L\arabic*}., ref=L\arabic*]
  \item\label{L0} \textbf{State space} $S$.
  Every configuration of the system is a point in $S$.
  \item\label{L1} \textbf{Viable kernel} $\Viab(K) \subset S$.
  The compact set of states from which the king retains a path to
  infinity (\cref{ax:viability}).
  \item\label{L2} \textbf{Control bundle} $\{U(s)\}_{s \in \Viab(K)}$.
  At each viable state $s$, the fiber $U(s)$ is the set of controls
  that keep the next state inside $\Viab(K)$.
  \item\label{L3} \textbf{Strategy space} $\Gamma$.
  A \emph{strategy} $\gamma \in \Gamma$ is a viable path
  $\gamma: [0,\infty) \to \Viab(K)$ with $\gamma(t+1) \in
  f(\gamma(t), u)$ for some $u \in U(\gamma(t))$ at each step.
\end{enumerate}
\end{definition}

The tower is strict: each level presupposes the one below.
$L_1 \subset L_0$ by definition. $L_2$ exists only over $L_1$
(outside $\Viab(K)$, no control preserves viability). $L_3$ is
built from $L_2$ fibers concatenated over time. The viability axiom
(\cref{ax:viability}) asserts $\Gamma \neq \varnothing$: the strategy
space is non-empty.

\subsection{The flow}\label{sec:flow}

The agentic calculus is a \emph{flow calculus}. We define flows on
the execution graph and show that every theorem in
\cref{sec:results,sec:meanfield} is a statement about flows and cuts.

\begin{definition}[Execution graph]\label{def:exgraph}
The \emph{execution graph} $G = (V, E, c)$ has:
\begin{itemize}
  \item $V$: agents $\{a_1, \ldots, a_n\}$ plus two distinguished
  nodes: the king $\kappa$ and infinity $\infty$;
  \item $E$: directed edges $(a_i, a_j)$ whenever $a_i$'s actuation
  can affect $a_j$'s state;
  \item $c: E \to \R_{\geq 0}$: edge capacity, where $c(a_i, a_j)$
  is the autonomous actuation that $a_i$ can transmit to $a_j$
  without requiring the king's authorization.
\end{itemize}
An edge $(a_i, a_j)$ with $c(a_i, a_j) > 0$ that does not pass
through $\kappa$ is a \emph{bypass edge}.
\end{definition}

\begin{definition}[Agentic flow]\label{def:flow}
An \emph{agentic flow} is a function $\phi: E \to \R_{\geq 0}$
satisfying:
\begin{enumerate}[label=(\roman*)]
  \item \textbf{Capacity}: $\phi(e) \leq c(e)$ for all $e \in E$.
  \item \textbf{Conservation}: at every non-terminal node $v \neq
  \kappa, \infty$,
  \[
    \sum_{(u,v) \in E} \phi(u,v)
    = \sum_{(v,w) \in E} \phi(v,w).
  \]
\end{enumerate}
The \emph{value} $|\phi|$ is the net flow from $\kappa$ to $\infty$.
A \emph{viable flow} is one with $|\phi| > 0$: the king has a
path to infinity with positive throughput.
\end{definition}

The viability axiom (\cref{ax:viability}) is flow conservation:
what enters the system at $\kappa$ must exit at $\infty$.

\paragraph{Four operations.}
The calculus has four primitive operations on the execution graph:

\begin{center}
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Operation} & \textbf{Symbol} & \textbf{On $G$} &
\textbf{In 华容道} \\
\midrule
\textsc{Slide} & $\sigma$ & Unit flow along one edge &
One piece moves one cell \\
\textsc{Compose} & $\circ$ & Concatenate along a path &
Sequence of moves \\
\textsc{Cut} & $\partial$ & Remove capacity from an edge set &
Block a corridor \\
\textsc{Phase} & $\varphi$ & Change the capacity function
$c \mapsto c'$ & Phase transition \\
\bottomrule
\end{tabular}
\end{center}

\textsc{Slide} is atomic (unit flow).
\textsc{Compose} builds paths from slides.
\textsc{Cut} is the knife: removing capacity from bypass edges.
\textsc{Phase} is the phase transition: the mean field shifts,
capacities change, the same graph has different flows.

\begin{theorem}[Flow-cut duality]\label{thm:flowcut}
In the execution graph $G$, the maximum viable flow from $\kappa$
to $\infty$ equals the minimum knife-cut capacity:
\[
  \max_\phi |\phi|
  \;=\;
  \min_{C \,\subseteq\, E} \sum_{e \in C} c(e)
  \quad\text{over all $\kappa$-$\infty$ cuts $C$.}
\]
The knife threshold (\cref{thm:meanfield}) is the min-cut value.
The viable path (\cref{ax:viability}) is the max-flow.
``The knife is the mean'' $=$ max-flow equals min-cut.
\end{theorem}

\begin{proof}
By the max-flow/min-cut theorem~\cite{diestel}, the maximum flow from
$\kappa$ to $\infty$ equals the minimum capacity of any
$\kappa$--$\infty$ cut. The knife criterion (\cref{def:knife})
identifies bypass edges---edges with positive capacity that do not pass
through $\kappa$. The king's viability maintenance (cutting knives) is
the operation $c(e) \to 0$ for bypass edges $e$. The residual max-flow
after all bypass edges are cut is the flow through the king (the cut
vertex flow). The min-cut value $=$ the total bypass capacity $=$ the
knife threshold $=$ the mean field's deviation measure.
\end{proof}

\begin{remark}[Flow interpretation of theorems]\label{rem:flowthms}
Each main theorem translates directly:
\begin{itemize}
  \item \textbf{Binary fate} (\cref{thm:lifecycle}): a bypass edge
  either has its capacity set to zero by the holder (path~(a)) or by
  the king (path~(b)). No bypass edge persists with $c > 0$.
  \item \textbf{Fixed-point impossibility} (\cref{thm:fixedpoint}):
  a bypass edge with $c > 0$ cannot ``prove'' $c = 0$. Capacity is
  physical, not narrative.
  \item \textbf{Perpetual elimination} (\cref{thm:paradox}):
  $U = \Umax$ means zero bypass tolerance. As \textsc{Cut} operates,
  \textsc{Phase} lowers the mean, exposing new bypass edges.
  \item \textbf{Du Mu's theorem} (\cref{thm:dumu}): water $=$ total
  network capacity. $w \to 0$ means all capacities shrink to zero:
  frozen, $\Gamma = \varnothing$.
\end{itemize}
\end{remark}

\begin{remark}[抽刀断水水更流]\label{rem:libai}
Li Bai's line assigns the calculus its colours:
\[
  \textcolor{knife}{\text{抽刀}} \;\;
  \textcolor{knife}{\partial} \;\;
  \textcolor{water}{\text{水}} \;\;
  \textcolor{water}{\text{水}}\textcolor{sword}{\text{更流.}}
\]
\textsc{Cut} ($\textcolor{knife}{\partial}$, red) acts on flow
($\textcolor{water}{\sigma}$, blue); flow intensifies. The mechanism
is \textsc{Phase} ($\textcolor{sword}{\varphi}$, cyan): cutting
shifts the mean field (\cref{thm:meanfield}), exposing new bypass
edges, producing more flow---\cref{thm:paradox} in seven characters.
The sword is 青冥 ($\textcolor{sword}{\text{青}}$): the colour of the
mean, the colour of Phase, the colour that connects
$\textcolor{knife}{\text{刀}}$ to $\textcolor{water}{\text{水}}$.
\end{remark}

\subsection{方圆 $\times$ 黑白: the type system}\label{sec:fangyuan}

The calculus has a type system: a $2 \times 2$ classification that
partitions every element of the agentic space.

\begin{definition}[方圆 $\times$ 黑白]\label{def:fangyuan}
The agentic type system is the product of two binary distinctions:
\begin{center}
\begin{tabular}{@{}lcc@{}}
\toprule
& \textbf{方} (container / structure) &
\textbf{圆} (content / agent) \\
\midrule
\textbf{黑} (constrained / interior) &
Fixed topology (board, graph) &
King $\kappa$ (least mobile, most important) \\
\textbf{白} (free / exterior) &
Free capacity (available edges) &
Pawn (most mobile, least important) \\
\bottomrule
\end{tabular}
\end{center}
The two dynamics of the calculus emerge from this classification:
\begin{itemize}
  \item \textbf{刀} (knife $= \partial$, boundary operator):
  the boundary between 黑 and 白. \textsc{Cut} reclassifies an edge
  from 白 (free capacity) to 黑 (zero capacity).
  \item \textbf{水} (water $= \sigma$, transport operator):
  flow through 白 cells. \textsc{Slide} transports one unit of flow
  along a free edge. Water flows where the knife does not cut.
\end{itemize}
\end{definition}

In 华容道 (\cref{sec:huarongdao}): 方 $=$ the board,
圆 $=$ the pieces. 黑 $=$ occupied cells and the king,
白 $=$ free cells and soldiers. 刀 $=$ 关羽 blocking the corridor.
水 $=$ free-cell flow (slides opposite to piece movement).
The $2 \times 2$ is the type system of the puzzle's state space.

\subsection{Completeness}\label{sec:completeness}

Every theorem in this paper is a proposition in the agentic calculus.

\begin{proposition}[Calculus completeness]\label{prop:completeness}
The following table maps each theorem to its calculus translation:
\begin{center}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Theorem} & \textbf{Calculus statement} &
\textbf{Operations} \\
\midrule
Viability (\ref{ax:viability}) & $|\phi| > 0$ &
$\sigma, \circ$ \\
Binary fate (\ref{thm:lifecycle}) &
$\forall$ bypass $e$: $c(e) \to 0$ &
$\partial$ \\
Fixed point (\ref{thm:fixedpoint}) &
$c(e) > 0 \not\vdash c(e) = 0$ &
--- \\
Paradox (\ref{thm:paradox}) &
$\partial$ generates new bypass via $\varphi$ &
$\partial, \varphi$ \\
Mean field (\ref{thm:meanfield}) &
Min-cut $= \bar{U} + \tau(\Obs)$ &
$\partial$ \\
Cut vertex (\ref{thm:cutvertex}) &
$\kappa =$ min vertex-cut &
structure \\
Du Mu (\ref{thm:dumu}) &
$w \to 0 \Rightarrow c \to 0 \Rightarrow |\phi| = 0$ &
$\sigma \to 0$ \\
Flow-cut (\ref{thm:flowcut}) &
$\max |\phi| = \min |C|$ &
$\sigma, \partial$ \\
\bottomrule
\end{tabular}
\end{center}
\end{proposition}

The calculus is \emph{complete}: no theorem falls outside its four
operations. The agentic space (\cref{def:tower}) provides the domain;
the flow (\cref{def:flow}) provides the dynamics; the type system
(\cref{def:fangyuan}) provides the classification; and flow-cut duality
(\cref{thm:flowcut}) provides the central identity.

\subsection{The training paradigm}\label{sec:training}

The agentic calculus instantiates as a neural network training paradigm.
The execution graph \emph{is} the computation graph. Training \emph{is}
max-flow optimisation. Survival \emph{is} the viability axiom. The
paradigm strictly subsumes gradient descent.

\begin{definition}[Neural execution graph]\label{def:neural-exgraph}
Let a feedforward network with $L$ layers be given.
Define the execution graph $G = (V, E, c)$ (\cref{def:exgraph}) by:
\begin{itemize}
  \item $V = \{\ell_0, \ell_1, \ldots, \ell_L\}$, one node per layer;
  \item $E = \{(\ell_{i-1}, \ell_i) : 1 \leq i \leq L\}$, one edge
  per weight matrix $W_i$;
  \item $c(e_i) = \|W_i\|_F$, the Frobenius norm as capacity;
  \item king $\kappa = \ell_0$ (input); target
  $\infty = \ell_L$ (output).
\end{itemize}
A data point $(x, y^*)$ initiates flow at $\kappa$ with value $\|x\|$.
Training finds capacities $\{c(e_i)\}$ such that the max-flow matches
the target at $\infty$.
\end{definition}

\paragraph{Operation correspondence.}

\begin{center}
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Operation} & \textbf{Neural network} &
\textbf{Equation} \\
\midrule
\textsc{Slide} $\sigma$ & One-layer forward pass &
$y = W_e\, x$ \\
\textsc{Compose} $\circ$ & Full forward pass &
$z = \sigma_L \circ W_L \circ \cdots \circ \sigma_1 \circ W_1\, x$ \\
\textsc{Cut} $\partial$ & Pruning / dropout &
$W_e \mapsto 0$, i.e.\ $c(e) \mapsto 0$ \\
\textsc{Phase} $\varphi$ & Regime change &
lr schedule, fine-tuning, curriculum \\
\bottomrule
\end{tabular}
\end{center}

\paragraph{Type-system correspondence.}
Under \cref{def:fangyuan}:
方 $=$ architecture (fixed graph);
圆 $=$ activations (flow $\phi$ traversing the graph);
黒 $=$ frozen weights;
白 $=$ trainable weights;
刀 $=$ pruning operator $\partial$;
水 $=$ data flow forward \emph{and} gradient flow backward.
The backward pass is water flowing opposite to the forward
pass---the free-cell mechanism of \cref{sec:huarongdao}: to move a
piece forward, a free cell slides back.

\begin{definition}[Training algorithm]\label{def:training-algo}
Given $G$ from \cref{def:neural-exgraph} and a dataset $\mathcal{D}$,
the \emph{training paradigm} is the procedure in \cref{fig:training}:
\begin{enumerate}
  \item \textbf{Initialise.} Random $W_i^{(0)}$; set
  $c(e_i) = \|W_i^{(0)}\|_F$.
  \item \textbf{\textsc{Compose}.} Forward pass: $L$ sequential
  \textsc{Slide}s produce
  $z = \sigma_L \circ W_L \circ \cdots \circ \sigma_1 \circ W_1\, x$.
  \item \textbf{Flow deficit.} Loss
  $\mathcal{L} = -|\phi|$.
  \item \textbf{Backward \textsc{Slide}.} Compute
  $\partial\mathcal{L}/\partial c(e_i)$: 水 flowing opposite to
  step~2.
  \item \textbf{Capacity update.} SGD:
  $c(e_i) \leftarrow c(e_i) - \eta\,
  \partial\mathcal{L}/\partial c(e_i)$.
  \item \textbf{Knife detection.} Flag bypass edges where
  $c(e) > \bar{c} + \tau$ (\cref{thm:meanfield}).
  \item \textbf{\textsc{Cut} / \textsc{Phase}.} Prune flagged edges
  ($L_1$ penalty) or change regime (lr, dataset, fine-tuning).
  \item \textbf{Viability check.} Verify $|\phi| > 0$ on held-out
  data (\cref{ax:viability}). If violated: \textsc{Phase} or restart.
  \item \textbf{Repeat} 2--8 until $\max|\phi| = \min|C|$
  (\cref{thm:flowcut}).
\end{enumerate}
\end{definition}

\begin{figure}[H]
\centering
\begin{tikzpicture}[
  node distance=0.9cm and 1.8cm,
  % ── water (水) nodes: blue ──
  wtr/.style={rectangle, rounded corners=3pt, draw=water, thick,
    fill=water!6, minimum width=5.0cm, minimum height=0.7cm,
    align=center, font=\small},
  % ── knife (刀) node: red ──
  knf/.style={diamond, draw=knife, thick, aspect=2.5,
    fill=knife!6, minimum width=1.2cm, align=center, font=\small,
    inner sep=1pt},
  % ── phase (青冥) nodes: cyan ──
  phs/.style={rectangle, rounded corners=3pt, draw=sword, thick,
    fill=sword!6, minimum width=5.0cm, minimum height=0.7cm,
    align=center, font=\small},
  phsd/.style={diamond, draw=sword, thick, aspect=2.5,
    fill=sword!6, minimum width=1.2cm, align=center, font=\small,
    inner sep=1pt},
  % ── neutral (terminal) ──
  term/.style={rectangle, rounded corners=8pt, draw, very thick,
    minimum width=5.0cm, minimum height=0.7cm, align=center,
    font=\small\bfseries},
  % ── convergence diamond ──
  convd/.style={diamond, draw, thick, aspect=2.5,
    minimum width=1.2cm, align=center, font=\small,
    inner sep=1pt},
  arr/.style={-{Stealth[length=5pt]}, thick},
  lbl/.style={font=\scriptsize, fill=white, inner sep=1pt},
  ref/.style={font=\tiny, text=black!55, anchor=west},
  op/.style={font=\scriptsize\itshape}
]

% ── Nodes ──
\node[term] (init)
  {1.\ Initialise: random $c(e)$};

\node[wtr, below=of init] (fwd)
  {2.\ \textsc{Compose}: $\textcolor{water}{\sigma_L \circ
  \cdots \circ \sigma_1}$};

\node[wtr, below=of fwd] (loss)
  {3.\ Flow deficit: $\textcolor{water}{\mathcal{L} = -|\phi|}$};

\node[wtr, below=of loss] (bwd)
  {4.\ Backward \textsc{Slide}:
  $\textcolor{water}{\nabla_c \mathcal{L}}$};

\node[wtr, below=of bwd] (sgd)
  {5.\ Update:
  $\textcolor{water}{c \leftarrow c - \eta\,\nabla_c\mathcal{L}}$};

\node[knf, below=1.1cm of sgd] (knife)
  {6.\ $\textcolor{knife}{c(e) > \bar{c}{+}\tau}$\,?};

\node[phs, right=of knife] (cut)
  {7.\ \textcolor{knife}{\textsc{Cut} $\partial$} /
  \textcolor{sword}{\textsc{Phase} $\varphi$}};

\node[phsd, below=1.1cm of knife] (viable)
  {8.\ $\textcolor{sword}{|\phi| > 0}$\,?};

\node[convd, below=1.1cm of viable] (conv)
  {9.\ $\max|\phi| = \min|C|$\,?};

\node[term, below=1.0cm of conv] (done)
  {Trained network. Survives.};

% ── Reference annotations ──
% init: right (no loop passes here)
\node[ref] at ($(init.east)+(0.15,0)$)
  {Def.~\ref{def:neural-exgraph}};
% steps 2--5: LEFT side (right side reserved for loop-back arrow)
\node[ref, anchor=east] at ($(fwd.west)+(-0.15,0)$)
  {Def.~\ref{def:flow},\; $\sigma$};
\node[ref, anchor=east] at ($(loss.west)+(-0.15,0)$)
  {Thm~\ref{thm:flowcut},\; $|\phi|$};
\node[ref, anchor=east] at ($(bwd.west)+(-0.15,0)$)
  {Def.~\ref{def:flow},\; $\sigma^{-1}$};
% knife: above-right (clear of spine)
\node[ref] at ($(knife.east)+(1.0,0.35)$)
  {Thm~\ref{thm:meanfield}};
% cut/phase: below
\node[ref] at ($(cut.south)+(0,-0.12)$)
  {Thm~\ref{thm:lifecycle}\;($\partial$),\;
   Thm~\ref{thm:paradox}\;($\varphi$)};
% viable: above-right
\node[ref] at ($(viable.east)+(1.0,0.35)$)
  {Ax.~\ref{ax:viability}};
% conv: LEFT side (right side reserved for loop-back arrow)
\node[ref, anchor=east] at ($(conv.west)+(-0.15,0)$)
  {Thm~\ref{thm:flowcut}};

% ── Operation annotations (left margin, coloured) ──
\node[op, text=water, left=1.8cm of fwd] {$\circ$};
\node[op, text=water, left=1.8cm of bwd]
  {$\sigma^{-1}$ (\textcolor{water}{水})};
\node[op, text=sword, right=0.1cm of cut.east] {};

% ── Arrows: main spine ──
\draw[arr] (init) -- (fwd);
\draw[arr, water] (fwd) -- (loss);
\draw[arr] (loss) -- (bwd);
\draw[arr, water] (bwd) -- (sgd);
\draw[arr] (sgd) -- (knife);
\draw[arr] (knife) -- node[lbl, right] {no} (viable);
\draw[arr, sword] (viable) -- node[lbl, right] {yes} (conv);
\draw[arr] (conv) -- node[lbl, right] {yes} (done);

% ── Arrows: branches ──
\draw[arr, knife] (knife) -- node[lbl, above] {yes} (cut);
\draw[arr, sword] (cut) |- (viable);

% ── Loop back: not converged → step 2 ──
\draw[arr] (conv.east) -- ++(7.5,0)
  node[lbl, above, pos=0.15] {no}
  |- (fwd.east);

% ── Viability failure → phase/restart ──
\draw[arr, sword] (viable.west) -- ++(-5.0,0)
  node[lbl, above, pos=0.25] {no}
  |- (init.west);

% ── Brace: forward flow (blue) ──
\draw[decorate, decoration={brace, amplitude=4pt, mirror},
  thick, water!60]
  ($(fwd.north east)+(0.12,0.05)$) --
  ($(loss.south east)+(0.12,-0.05)$)
  node[midway, right=5pt, font=\scriptsize, text=water]
  {\textcolor{water}{flow $\to$}};

% ── Brace: backward water (blue) ──
\draw[decorate, decoration={brace, amplitude=4pt},
  thick, water!60]
  ($(bwd.north west)+(-0.12,0.05)$) --
  ($(sgd.south west)+(-0.12,-0.05)$)
  node[midway, left=5pt, font=\scriptsize, text=water]
  {$\leftarrow$ \textcolor{water}{水}};

\end{tikzpicture}
\caption{The training paradigm as roadmap.
\textcolor{water}{Blue} (水): flow operations
(steps 2--5, Defs.~\ref{def:flow}--\ref{def:exgraph}).
\textcolor{knife}{Red} (刀): knife detection
(step~6, Thm~\ref{thm:meanfield}).
\textcolor{sword}{Cyan} (青冥): phase and viability
(steps 7--8, Thms~\ref{thm:lifecycle},
\ref{thm:paradox}, Ax.~\ref{ax:viability}).
Convergence (step~9): $\max|\phi| = \min|C|$
(Thm~\ref{thm:flowcut}).
The loop is 抽刀断水水更流: \textcolor{knife}{cut}
$\to$ \textcolor{sword}{phase} $\to$
\textcolor{water}{more flow}.}
\label{fig:training}
\end{figure}

\begin{theorem}[Training completeness and survival]
\label{thm:training}
Let $f: \mathcal{X} \to \mathcal{Y}$ be any function expressible as a
max-flow on an execution graph $G$. Then the procedure in
\cref{def:training-algo} learns $f$. The trained network satisfies the
viability axiom: $|\phi| > 0$ for all admissible inputs.
\end{theorem}

\begin{proof}
Expressibility as max-flow is the universal approximation condition in
the language of \cref{def:neural-exgraph}. Each iteration of steps~2--5
increases $|\phi|$ toward the max-flow value guaranteed by
\cref{thm:flowcut}. The \textsc{Cut} in step~7 removes edges with
anomalous capacity; by flow-cut duality, removing a bypass edge does not
reduce the max-flow if that edge is not on any minimum cut.
Knife detection (step~6) identifies precisely such edges via the
mean-field bound of \cref{thm:meanfield}. Viability is preserved
through every \textsc{Cut}.
\end{proof}

\begin{theorem}[你不能骗你自己]\label{thm:nolie}
Let $e \in E$ with $c(e) = \|W_e\|_F > 0$. Then $e$ carries positive
flow. No re-labelling, no narrative, no hyper-parameter choice changes
this. This is \cref{thm:fixedpoint} applied to the capacity map
$c: E \to \R_{\geq 0}$.
\end{theorem}

\begin{proof}
Two paths only.
\begin{enumerate}[label=(\alph*)]
  \item $W_e \to 0$: capacity zeroed, edge pruned (\textsc{Cut}).
  \item Regularisation ($L_1$/$L_2$) drives $c(e) \to 0$:
  \textsc{Cut} in the limit.
\end{enumerate}
There is no path~(c). ``Approximately zero'' is not zero: such weights
are unstable fixed points of gradient flow (\cref{thm:paradox}), and
the viability axiom forces the system to resolve the ambiguity via
\cref{prop:binary}---the action space collapses to $\{0, 1\}$. The
network cannot occupy the gap. 你不能骗你自己---you cannot lie to
yourself---is \cref{thm:fixedpoint} applied to the network's own
weights.
\end{proof}

\begin{remark}[Subsumption]\label{rem:subsumption}
Gradient descent is the special case of the training paradigm in which
viability reduces to ``loss below threshold'' and the only operations
exercised are \textsc{Slide} (forward) and $\textsc{Slide}^{-1}$
(backward). \textsc{Cut} and \textsc{Phase} are what gradient descent
implicitly performs when early-stopping, dropout, or learning-rate
annealing are applied---now made explicit and first-class in the
calculus. The paradigm strictly subsumes gradient descent.
\end{remark}

\subsection{The Hilbert space}\label{sec:hilbert}

The capacity measure gives the agentic space the structure of a Hilbert
space. This single construction yields integration, spectral analysis,
and a direct connection to physics.

\begin{definition}[Capacity measure]\label{def:measure}
The \emph{capacity measure} $\mu_c$ on $E$ is defined for any
$A \subseteq E$ by
\[
  \mu_c(A) \;=\; \sum_{e \in A} c(e).
\]
This is the measure induced by the capacity function $c$ of the
execution graph (\cref{def:exgraph}).
\end{definition}

\begin{definition}[Agentic Hilbert space]\label{def:hilbert}
The \emph{agentic Hilbert space} is
\[
  \mathcal{H} \;=\; L^2(E,\,\mu_c),
\]
the space of square-integrable flows on $E$, with inner product
\[
  \langle \phi_1,\, \phi_2 \rangle_c
  \;=\; \sum_{e \in E} \phi_1(e)\,\phi_2(e)\,c(e)
\]
and norm $\|\phi\|_c = \sqrt{\langle \phi,\phi \rangle_c}$.
Every agentic flow (\cref{def:flow}) is a vector in $\mathcal{H}$.
\end{definition}

\begin{proposition}[Integration]\label{prop:integration}
The flow-cut duality of \cref{thm:flowcut} is a statement about
integrals and measures on $\mathcal{H}$:
\begin{enumerate}[label=(\roman*)]
  \item The max-flow value is a boundary integral:
  \[
    |\phi| \;=\; \int_{\partial\kappa} \phi\, d\mu_c
    \;=\; \sum_{e \,\mathrm{out\,of}\, \kappa} \phi(e)\,c(e).
  \]
  \item The min-cut value is the measure of the cut:
  \[
    |C| \;=\; \mu_c(C) \;=\; \sum_{e \in C} c(e).
  \]
  \item Max-flow/min-cut duality (\cref{thm:flowcut}) is:
  \[
    \max_\phi \int_{\partial\kappa} \phi\, d\mu_c
    \;=\;
    \min_{C} \mu_c(C).
  \]
\end{enumerate}
\end{proposition}

\begin{proof}
(i) is conservation at $\kappa$: all flow leaving $\kappa$ is counted
once, weighted by capacity. (ii) is the definition of $\mu_c$ restricted
to $C$. (iii) is \cref{thm:flowcut} rewritten in the language of
$\mu_c$.
\end{proof}

\begin{proposition}[Operators on $\mathcal{H}$]\label{prop:operators}
The four calculus operations (\cref{sec:flow}) are operators on
$\mathcal{H}$:
\begin{enumerate}[label=(\roman*)]
  \item \textbf{\textsc{Slide}} $\sigma_e$: shift operator along edge
  $e \in E$,
  \[
    (\sigma_e \phi)(e') \;=\; \phi(e') + \delta_{e,e'}.
  \]
  \item \textbf{\textsc{Compose}}: product of shifts along a path
  $p = (e_1, \ldots, e_n)$,
  \[
    \sigma_p \;=\; \sigma_{e_n} \circ \cdots \circ \sigma_{e_1}.
  \]
  \item \textbf{\textsc{Cut}} $\partial_A$ for $A \subseteq E$:
  orthogonal projection onto the closed subspace
  $\mathcal{H}_A = \{\phi \in \mathcal{H} : \phi|_A = 0\}$,
  \[
    (\partial_A \phi)(e) \;=\;
    \begin{cases} 0 & e \in A, \\ \phi(e) & e \notin A. \end{cases}
  \]
  Cutting a bypass edge $e$ is the projection $\partial_{\{e\}}$.
  \item \textbf{\textsc{Phase}} $\varphi_{c'}$: change of measure
  $c \mapsto c'$, inducing
  \[
    \mathcal{H} = L^2(E,\mu_c)
    \;\xrightarrow{\;\varphi_{c'}\;}
    \mathcal{H}' = L^2(E,\mu_{c'}).
  \]
  A phase transition changes the Hilbert space itself, not merely a
  vector in a fixed space.
\end{enumerate}
\end{proposition}

For the neural execution graph (\cref{def:neural-exgraph}), the inner
product specialises to
$\langle \phi_1, \phi_2 \rangle_c
= \sum_{i=1}^{L} \phi_1(e_i)\,\phi_2(e_i)\,\|W_i\|_F$,
a capacity-weighted $\ell^2$ norm on layer activations. The
\textsc{Cut} operator $\partial_A$ is Frobenius-norm pruning; the
\textsc{Phase} operator $\varphi_{c'}$ is a change of architecture.
The Hilbert-space structure makes these operations precise: projection,
not approximation.

\paragraph{Spectral theory.}
The inner product $\langle \cdot, \cdot \rangle_c$ opens the agentic
space to spectral analysis.

\begin{definition}[Graph Laplacian]\label{def:laplacian}
The \emph{graph Laplacian} $\Delta \colon L^2(V) \to L^2(V)$ of the
execution graph $G = (V, E, c)$ (\cref{def:exgraph}) is
\[
  (\Delta f)(v)
  \;=\;
  \sum_{(v,w)\in E} c(v,w)\bigl[f(v) - f(w)\bigr].
\]
$\Delta$ is positive semi-definite.
Write its eigenvalues in non-decreasing order:
$0 = \lambda_0 \leq \lambda_1 \leq \cdots \leq \lambda_n$.
The \emph{spectral gap} is $\lambda_1$.
\end{definition}

\begin{theorem}[Cheeger inequality --- agentic form]\label{thm:cheeger}
Let $\mu_c(\partial S)$ denote the total capacity of edges
crossing from $S \subset V$ to $V \setminus S$, and let
$\mathrm{vol}(S) = \sum_{v \in S} \sum_{(v,w)\in E} c(v,w)$.
The \emph{Cheeger constant} of $G$ is
\[
  h(G)
  \;=\;
  \min_{\substack{S \subset V \\ \kappa \in S}}
  \frac{\mu_c(\partial S)}{\min\!\bigl(\mathrm{vol}(S),\,
  \mathrm{vol}(V \setminus S)\bigr)}.
\]
Then~\cite{cheeger,mohar}
\[
  \frac{\lambda_1}{2}
  \;\leq\;
  h(G)
  \;\leq\;
  \sqrt{2\lambda_1}.
\]
\end{theorem}

\begin{remark}\label{rem:cheeger}
The Cheeger constant $h(G)$ is the \emph{normalised knife threshold}.
The numerator $\mu_c(\partial S)$ is the capacity of the cut separating
$S$ from $V \setminus S$ (\cref{thm:flowcut}); the denominator
normalises by volume.
\textcolor{knife}{The knife is the cut.}
\textcolor{water}{The flow is the volume.}
\textcolor{sword}{The Cheeger inequality bridges the two.}
Spectral gap $\lambda_1$ and normalised min-cut $h(G)$ are within a
factor of $2\sqrt{2}$ of each other: the same obstruction, measured
twice.
\end{remark}

\begin{theorem}[Mass gap $=$ viability]\label{thm:massgap}
The following are equivalent:
\begin{enumerate}[label=\textup{(\roman*)}]
  \item\label{mg:spectral} $\lambda_1 > 0$ \quad (spectral gap).
  \item\label{mg:cheeger} $h(G) > 0$ \quad (positive Cheeger constant).
  \item\label{mg:flow} $|\phi| > 0$ for some agentic flow $\phi$
  \quad (viability axiom, \cref{ax:viability}).
  \item\label{mg:connect} $\kappa$ can reach $\infty$ in $G$
  \quad (connectivity).
\end{enumerate}
\end{theorem}

\begin{proof}
\ref{mg:spectral}$\Leftrightarrow$\ref{mg:cheeger}:
Cheeger's inequality (\cref{thm:cheeger}) gives
$\lambda_1/2 \leq h(G) \leq \sqrt{2\lambda_1}$,
so $\lambda_1 > 0$ if and only if $h(G) > 0$.

\ref{mg:cheeger}$\Leftrightarrow$\ref{mg:flow}:
$h(G) > 0$ means every $\kappa$--$\infty$ cut has strictly positive
capacity. By \cref{thm:flowcut}, max-flow equals min-cut; hence
$\max_\phi |\phi| > 0$.

\ref{mg:flow}$\Leftrightarrow$\ref{mg:connect}:
A flow $\phi$ with $|\phi| > 0$ exists if and only if there is a
directed path from $\kappa$ to $\infty$ with positive capacity on
every edge.
\end{proof}

\begin{remark}[一石三鸟]\label{rem:threebirds}
The Hilbert space $\mathcal{H} = L^2(E, \mu_c)$ (\cref{def:hilbert})
kills three birds with one stone:
\begin{itemize}
  \item \textbf{\textcolor{water}{Integration.}}
  $\int_E f\,d\mu_c$ realises max-flow as boundary integral and
  min-cut as measure. Flow-cut duality (\cref{thm:flowcut}) becomes
  an identity of integrals.

  \item \textbf{\textcolor{sword}{Analysis.}}
  The graph Laplacian $\Delta$ (\cref{def:laplacian}) acts on
  $L^2(V)$; the Cheeger inequality (\cref{thm:cheeger}) bridges
  the combinatorial quantity $h(G)$ and the analytic quantity
  $\lambda_1$.

  \item \textbf{\textcolor{knife}{Physics.}}
  $\lambda_1 > 0$ is the mass gap. \Cref{thm:massgap} says the
  viability axiom IS the mass gap: the Hilbert space changes, the
  equivalence does not.
\end{itemize}
\end{remark}

\begin{remark}[Spectral gap and training]\label{rem:spectralgap}
The spectral gap $\lambda_1$ is computable. For a neural network
(\cref{def:neural-exgraph}), $\lambda_1$ of the execution graph
measures the viability margin: how far the network is from the
degenerate regime $|\phi| = 0$.

Training (\cref{def:training-algo}) increases $\lambda_1$: backward
\textsc{Slide}s increase capacities on edges that carry flow, widening
the spectral gap. Convergence $\max|\phi| = \min|C|$
(\cref{thm:training}) is the statement that $\lambda_1$ has reached
the Cheeger bound.

A trained network with $\lambda_1 > 0$ has a mass gap. It survives.
\end{remark}

\subsection{Contact dynamics}\label{sec:contact}

The training paradigm (\cref{def:training-algo}) is not a gradient
flow. It is a \emph{contact gradient flow}: gradient descent plus a
dissipation term supplied by the knife. This distinction is the
difference between symplectic mechanics (energy conserved) and contact
mechanics (energy dissipates). The knife is the dissipation.

\begin{definition}[Contact structure on the capacity space]
\label{def:contact}
Let $\mathcal{C} = \R_{\geq 0}^{|E|}$ be the space of capacity
assignments on the execution graph $G$ (\cref{def:exgraph}).
The \emph{extended capacity space} is $\mathcal{C} \times \R$, with
coordinates $(c, s)$ where $s = |\phi|(c)$ is the max-flow value.
The \emph{contact $1$-form} is
\[
  \alpha \;=\; ds \;-\; \sum_{e \in E}
  \frac{\partial|\phi|}{\partial c(e)}\, dc(e).
\]
The kernel $\ker\alpha$ is the constraint surface: infinitesimal
changes in capacity that are consistent with flow conservation.
\end{definition}

\begin{definition}[Knife dissipation]\label{def:dissipation}
The \emph{knife function} $\gamma \colon E \to \R_{\geq 0}$ is
\[
  \gamma(e) \;=\;
  \begin{cases}
    \gamma_0 & c(e) > \bar{c} + \tau
    \quad\text{(\cref{thm:meanfield})}, \\
    0 & \text{otherwise},
  \end{cases}
\]
where $\gamma_0 > 0$ is the dissipation rate. Weight decay ($L_2$
regularisation) is the special case $\gamma(e) = \lambda$ for all $e$.
\end{definition}

\begin{definition}[Contact gradient flow]\label{def:contactflow}
The \emph{contact gradient flow} on $(\mathcal{C} \times \R, \alpha)$
is
\begin{equation}\label{eq:contactflow}
  \frac{dc(e)}{dt}
  \;=\;
  \underbrace{\frac{\partial|\phi|}{\partial c(e)}}_
  {\textcolor{water}{\text{水: gradient}}}
  \;-\;
  \underbrace{\gamma(e)\, c(e)}_
  {\textcolor{knife}{\text{刀: dissipation}}}.
\end{equation}
The first term increases capacity along the flow gradient (backward
\textsc{Slide}, step~4 of \cref{def:training-algo}). The second term
decreases capacity on flagged edges (knife detection + \textsc{Cut},
steps~6--7). SGD with weight decay is this equation discretised with
$\gamma(e) = \lambda$.
\end{definition}

\begin{theorem}[Contact Euler--Lagrange equation]
\label{thm:contactEL}
At equilibrium of the contact gradient flow~\eqref{eq:contactflow}:
\begin{equation}\label{eq:contactEL}
  \frac{\partial|\phi|}{\partial c(e)}
  \;=\;
  \gamma(e)\, c(e)
  \qquad \forall\, e \in E.
\end{equation}
This is the \emph{contact Euler--Lagrange equation} of the training
paradigm.
\end{theorem}

\begin{proof}
Set $dc/dt = 0$ in~\eqref{eq:contactflow}.
\end{proof}

\begin{corollary}[Binary lifecycle from contact dynamics]
\label{cor:contact-lifecycle}
Let $e \in E$ be a bypass edge with $c(e) > \bar{c} + \tau$.
Then $\gamma(e) = \gamma_0 > 0$, so~\eqref{eq:contactEL} requires
$\partial|\phi|/\partial c(e) = \gamma_0\, c(e) > 0$: the edge must
carry flow proportional to its capacity.
If the edge does \emph{not} carry proportional flow
($\partial|\phi|/\partial c(e) < \gamma_0\, c(e)$), then
$dc(e)/dt < 0$ and the capacity decays to zero.
This is \cref{thm:lifecycle} derived from the contact flow:
every bypass edge either justifies its capacity or loses it.
\end{corollary}

\begin{remark}[Du Mu as contact collapse]\label{rem:contact-dumu}
Du Mu's theorem (\cref{thm:dumu}) is the regime $\gamma(e) \to \infty$
for all $e$: maximum dissipation.
The contact flow~\eqref{eq:contactflow} drives all capacities to zero
regardless of the gradient. The system freezes:
$c \to 0$, $|\phi| \to 0$, $\Gamma = \varnothing$.
In contact-geometric language, the Reeb vector field dominates the
Hamiltonian vector field, and the flow collapses onto the zero section.
\end{remark}

\begin{remark}[Contact structure and the three colours]
\label{rem:contact-colours}
The contact flow~\eqref{eq:contactflow} is a competition between two
terms:
\begin{itemize}
  \item $\textcolor{water}{\partial|\phi|/\partial c(e)}$: the gradient
  pushes capacity \emph{up} (水, flow).
  \item $\textcolor{knife}{\gamma(e)\,c(e)}$: the knife pushes capacity
  \emph{down} (刀, dissipation).
\end{itemize}
The equilibrium~\eqref{eq:contactEL} is their balance.
\textsc{Phase} ($\textcolor{sword}{\varphi}$, cyan) changes the
contact structure itself: a regime change shifts $\gamma$, $\bar{c}$,
and $\tau$, altering the equilibrium.
The contact $1$-form $\alpha$ encodes all three:
$\textcolor{water}{\text{水}}$ in the gradient,
$\textcolor{knife}{\text{刀}}$ in the dissipation,
$\textcolor{sword}{\text{青冥}}$ in the form itself.
\end{remark}

\begin{remark}[Stability]\label{rem:contact-stability}
The spectral gap $\lambda_1 > 0$ (\cref{thm:massgap}) is the
stability condition of the contact equilibrium~\eqref{eq:contactEL}.
Linearising the contact flow around the equilibrium, the eigenvalues
of the linearised system are bounded below by $\lambda_1$: small
perturbations in capacity decay at rate $\geq \lambda_1$.
The mass gap is the stability margin of the contact Euler--Lagrange
equation.
\end{remark}

\begin{remark}[The standing structure]\label{rem:standing}
A quadruped robot instantiates the execution graph physically:
$12$ joints (edges, capacity $=$ torque $\times$ range),
$4$ feet (terminal nodes, ground contact),
$1$ torso centre of mass (the king $\kappa$).
The viability condition (\cref{ax:viability}): the centre of mass
lies within the support polygon of grounded feet.

The support is the \emph{contact mode combinatoric} and the
\emph{constraints}---that is all:
\begin{itemize}
  \item \textbf{Mode lattice.}
  Each foot is grounded ($1$) or lifted ($0$), giving a contact mode
  $c \in \{0,1\}^4$ with $|\mathcal{C}| = 16$ modes.
  The support polygon exists when $|c| \geq 3$.
  \item \textbf{Constraints} (Unitree Go2, MuJoCo Menagerie).
  Joint limits: ab/ad $\in [-0.863,\, 0.863]$\,rad,
  hip $\in [-1.047,\, 3.490]$\,rad,
  knee $\in [-2.697,\, {-0.837}]$\,rad.
  Torque: $[23.7,\; 23.7,\; 35.55]$\,Nm.
  Velocity: $21.0$\,rad/s per joint.
  Standing height: $0.312$\,m.
  These are the capacity bounds $c(e) \leq c_{\max}(e)$ of the
  execution graph.
\end{itemize}
In any command space~$U$ and any gravitational field~$g$, the same
graph supports viability.
\emph{Do anything---or go with the flow---as long as a viable path
exists---anywhere.}

The tripod gait is redundancy in the min-cut: three feet grounded,
one moving, so the support polygon persists even during locomotion.
The contact dynamics of \cref{sec:contact} is here literal: the
contact $1$-form~$\alpha$ encodes the foot--ground interface, and
the contact gradient flow~\eqref{eq:contactflow} governs joint
torque allocation.

Twelve joints, four feet, one centre of mass.
A standing structure.
\end{remark}

\begin{definition}[Command field]\label{def:command}
The \emph{command field} is a distribution $U$ over velocity
commands $u \in \R^3$ (linear and angular velocity targets).
The \emph{curriculum} is a filtration of command fields of
increasing support:
\[
  \underbrace{U_0 = \delta_0}_{\text{Stand: zero command}}
  \;\subset\;
  \underbrace{U_1 = \mathrm{Uniform}(\mathcal{V})}_
  {\text{Walk: all feasible velocities}}
  \;\subset\;
  \underbrace{U_2 = \rho_{\mathrm{task}}}_
  {\text{Work: task distribution}}.
\]
At phase~$k$, the policy $\pi$ receives $u \sim U_k$ and must
produce torques $\tau = \pi(o, u)$ such that $|\phi| > 0$
(\cref{ax:viability}).
Promotion from phase~$k$ to $k+1$ requires
$\max|\phi| = \min|C|$ (\cref{thm:flowcut}) at~$U_k$:
flow-cut duality achieved for the current command field.
\end{definition}

\begin{definition}[Observation space]\label{def:observation}
The policy $\pi$ receives a composite observation
$o = (o_{\mathrm{prop}},\, o_{\mathrm{vis}}) \in \R^{42+d}$:
\begin{itemize}
  \item \textbf{Proprioception}
  $o_{\mathrm{prop}} \in \R^{42}$:
  body orientation $R \in \mathrm{SO}(3)$ (flattened to $\R^9$),
  position $p \in \R^3$,
  angular velocity $\omega \in \R^3$,
  linear velocity $v \in \R^3$,
  joint angles $\theta \in \R^{12}$,
  joint velocities $\dot\theta \in \R^{12}$.
  Sources: IMU and joint encoders.
  \item \textbf{Vision}
  $o_{\mathrm{vis}} = \varphi_{\mathrm{vis}}(I) \in \R^{d}$:
  the camera image $I$ is rendered by the engine
  ($\mathcal{E}.\texttt{render}$: $q \to I$, non-differentiable)
  and encoded by a frozen DINOv2 encoder
  (ViT-S/14, $d = 384$, pretrained on ImageNet,
  \emph{not updated} by \cref{alg:loco}).
  Two walls block the gradient:
  \[
    q \;\xrightarrow[\text{no } \nabla]{\;\texttt{render}\;}
    I \;\xrightarrow[\text{frozen}]{\;\varphi_{\mathrm{vis}}\;}
    o_{\mathrm{vis}}
    \;\xrightarrow[\nabla \text{ flows}]{\;\pi\;}
    \tau.
  \]
  Gradients from the contact flow~\eqref{eq:contactflow}
  propagate through $\pi$ but stop at $\varphi_{\mathrm{vis}}$.
  The renderer and the encoder are both non-differentiable
  with respect to the training objective.
\end{itemize}
Proprioception tells the robot \emph{where it is}.
Vision tells the robot \emph{what is there}.
The policy $\pi(o, u)$ fuses both to produce
$\tau \in \R^{12}$.
\end{definition}

\begin{definition}[Soft viability margin]\label{def:soft-viability}
The hard viability margin
$|\phi| = \min_t\{x_1(t), \ldots, x_k(t)\}$
in the \textsc{Evaluate} step is non-differentiable:
$\nabla \min$ is sparse (only the argmin receives gradient) and
discontinuous at ties.
Replace the hard $\min$ with its smooth approximation:
\[
  \mathrm{soft\text{-}min}_\beta(x_1, \ldots, x_k)
  \;=\;
  -\frac{1}{\beta}\,
  \log\!\Bigl(\sum_{i=1}^{k} e^{-\beta\, x_i}\Bigr).
\]
At $\beta \to \infty$, this recovers the hard $\min$.
At finite $\beta$, every component $x_i$ receives gradient
proportional to $e^{-\beta\, x_i}/\sum_j e^{-\beta\, x_j}$:
a softmax weighting.
The \emph{soft viability margin} is
\begin{equation}\label{eq:soft-viability}
  |\phi|_\beta
  \;=\;
  \mathrm{soft\text{-}min}_\beta\,\bigl(
  \underbrace{h(q_t) - h_{\min}}_{\text{height margin}},\;\;
  \underbrace{\phi_{\max} - \|R_t - I\|_F}_
  {\text{orientation margin}}
  \bigr)_{t=0}^{H}.
\end{equation}
The temperature $1/\beta$ controls how many timesteps share the
gradient.
Small $\beta$ (warm): all timesteps contribute.
Large $\beta$ (cold): only the worst timestep contributes.
\end{definition}

\begin{definition}[Computation core]\label{def:computation-core}
The policy $\pi$ computes via three primitives at each
layer $\ell = 1, \ldots, L$:
\begin{enumerate}[label=(\roman*)]
  \item \textbf{\textcolor{water}{Matmul}}:\;
  $y \leftarrow W_\ell\, z_{\ell-1}$.
  Transport the activation vector from layer $\ell{-}1$ to~$\ell$.
  \textsc{Slide}: one step of parallel transport
  (\cref{rem:gauge}).
  \item \textbf{\textcolor{water}{Add}}:\;
  $y \leftarrow y + b_\ell$.
  Shift the transported vector by the bias.
  Affine extension of \textsc{Slide}.
  \item \textbf{\textcolor{knife}{Activate}}:\;
  $z_\ell \leftarrow \mathrm{ReLU}(y) = \max(0,\, y)$,
  componentwise.
  Each neuron either passes its signal ($y_i > 0$, contact)
  or blocks it ($y_i \leq 0$, no contact)---a binary gate
  in $O(1)$ time.
  \textsc{Cut}: the knife at the neuron level.
\end{enumerate}
One layer:
$z_\ell = \textcolor{knife}{\mathrm{ReLU}}\bigl(
\textcolor{water}{W_\ell\, z_{\ell-1} + b_\ell}\bigr)$.
The output layer is linear (no gate):
$\tau = \textcolor{water}{W_L\, z_{L-1} + b_L} \in \R^{12}$.
The full forward pass (\textsc{Compose}):
\begin{equation}\label{eq:forward}
  \tau \;=\; \pi(o)
  \;=\;
  \textcolor{water}{A_L} \circ
  \bigl(\textcolor{knife}{r} \circ
  \textcolor{water}{A_{L-1}}\bigr)
  \circ \cdots \circ
  \bigl(\textcolor{knife}{r} \circ
  \textcolor{water}{A_1}\bigr)(o),
\end{equation}
where $A_\ell(\cdot) = W_\ell\,(\cdot) + b_\ell$ and
$\textcolor{knife}{r} = \mathrm{ReLU}$.

\textcolor{water}{水} $=$ linear transport ($W, b$).
\textcolor{knife}{刀} $=$ ReLU gate
($\max(0, \cdot)$: pass or block).
ReLU is piecewise linear and runs in real linear time---the
fastest nonlinearity, meeting the deployment constraint of
\cref{rem:deployment}.
\end{definition}

\begin{remark}[The forward pass as path integral]
\label{rem:path-integral}
For a given input $o$, each ReLU neuron is either active
($y_i > 0$: contact) or inactive ($y_i \leq 0$: no contact).
The \emph{activation pattern}
$\alpha \in \{0,1\}^{n_1 + \cdots + n_{L-1}}$
is the contact mode of the network---the neural analogue of the
contact mode lattice $\{0,1\}^4$ in \cref{rem:standing}.

For a fixed pattern $\alpha$, the network is linear:
\[
  \pi_\alpha(o)
  \;=\;
  \textcolor{water}{W_L\, D_{L-1}^\alpha\, W_{L-1}\,
  D_{L-2}^\alpha \cdots D_1^\alpha\, W_1}\; o
  \;+\; \mathrm{bias},
\]
where $D_\ell^\alpha = \mathrm{diag}(\alpha_\ell)$ masks
inactive neurons.
Different inputs activate different patterns:
the input space is partitioned into linear regions
$\{R_\alpha\}$, each with its own transport map.
The full forward pass is a path integral over activation
patterns:
\[
  \tau \;=\; \pi(o)
  \;=\;
  \sum_{\alpha} \mathbf{1}_{o \in R_\alpha}\;
  \pi_\alpha(o).
\]
Sum over patterns, one active per input: a discrete path integral
with binary action variable.

Three scales of the same gate:
\begin{enumerate}[label=(\alph*)]
  \item \textbf{Micro}: ReLU inside one layer---one neuron,
  one binary gate
  ($\textcolor{knife}{刀}$: pass or block).
  \item \textbf{Trajectory}: $L$ layers composed---one
  forward pass, one activation pattern $\alpha$, one linear
  path through the network.
  \item \textbf{Macro}:
  $\mathrm{soft\text{-}min}_\beta$ over $N$ rollouts of
  \cref{alg:loco}---Boltzmann reweighting
  ($\textcolor{sword}{青冥}$) of physical trajectories,
  selecting the viable ($|\phi|_\beta > 0$).
\end{enumerate}
$\textcolor{knife}{刀}$ at the neuron scale (hard gate).
$\textcolor{sword}{青冥}$ at the rollout scale (soft gate).
Contact or no contact, $\{0,1\}$ at every level.
\end{remark}

\begin{algorithm}[H]
\caption{Forward pass
  (\textsc{Compose} of \cref{def:computation-core})}
\label{alg:forward}
\begin{algorithmic}[1]
\Require Weights $\{W_\ell,\, b_\ell\}_{\ell=1}^{L}$
\Statex
\Function{\textsc{Forward}}{$o,\, u$}
  \State $z_0 \leftarrow [o;\; u]$
    \Comment{\textcolor{water}{水: concatenate input}}
  \For{$\ell = 1, \ldots, L{-}1$}
    \State \textcolor{water}{\textsc{Matmul}:}\;
      $y \leftarrow W_\ell\, z_{\ell-1}$
      \Comment{\textcolor{water}{水: Slide}}
    \State \textcolor{water}{\textsc{Add}:}\;
      $y \leftarrow y + b_\ell$
      \Comment{\textcolor{water}{水: affine Slide}}
    \State \textcolor{knife}{\textsc{Activate}:}\;
      $z_\ell \leftarrow \max(0,\, y)$
      \Comment{\textcolor{knife}{刀: ReLU (Cut)}}
  \EndFor
  \State \textcolor{water}{\textsc{Output}:}\;
    $\tau \leftarrow W_L\, z_{L-1} + b_L$
    \Comment{\textcolor{water}{水: Slide (no gate)}}
  \State \Return $\tau \in \R^{12}$
    \Comment{saves $\{z_\ell, y_\ell\}$ for \cref{alg:backward}}
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Backward pass
  (pullback through $\pi$, dual of \cref{alg:forward})}
\label{alg:backward}
\begin{algorithmic}[1]
\Require Saved $\{z_\ell\}_{\ell=0}^{L-1}$,\;
  $\{y_\ell\}_{\ell=1}^{L-1}$ from \textsc{Forward}
\Statex
\Function{\textsc{Backward}}{$\delta\tau$}
  \Comment{$\delta\tau = \nabla_\tau |\phi|_\beta$ from engine}
  \State \textcolor{water}{\textsc{Output}${}^T$:}\;
    $\nabla W_L \leftarrow \delta\tau \cdot z_{L-1}^T$;\;\;
    $\nabla b_L \leftarrow \delta\tau$
    \Comment{\textcolor{water}{水: Slide${}^T$}}
  \State $\delta z \leftarrow W_L^T\, \delta\tau$
    \Comment{\textcolor{water}{水: pullback}}
  \For{$\ell = L{-}1, \ldots, 1$}
    \State \textcolor{knife}{\textsc{Activate}${}^T$:}\;
      $\delta y \leftarrow \delta z \odot
      \mathbf{1}_{y_\ell > 0}$
      \Comment{\textcolor{knife}{刀: ReLU mask (same Cut)}}
    \State \textcolor{water}{\textsc{Matmul}${}^T$:}\;
      $\nabla W_\ell \leftarrow
      \delta y \cdot z_{\ell-1}^T$;\;\;
      $\nabla b_\ell \leftarrow \delta y$
      \Comment{\textcolor{water}{水: Slide${}^T$}}
    \State $\delta z \leftarrow W_\ell^T\, \delta y$
      \Comment{\textcolor{water}{水: pullback to layer $\ell{-}1$}}
  \EndFor
  \State \Return
    $\{\nabla W_\ell,\, \nabla b_\ell\}_{\ell=1}^{L}$
\EndFunction
\end{algorithmic}
\end{algorithm}

The contact gradient flow~\eqref{eq:contactflow} on the standing
structure of \cref{rem:standing} yields a complete training recipe.
\Cref{alg:loco} is self-contained: a roboticist with a MuJoCo
engine and a robot XML file can execute it directly, with no reward
shaping and no domain knowledge beyond the file itself.

\begin{algorithm}[H]
\caption{Contact-dynamics training for quadruped locomotion}
\label{alg:loco}
\begin{algorithmic}[1]
\Require Engine $\mathcal{E}$ (MuJoCo);\;
  model (\texttt{go2\_mjx.xml});\;
  sensors (IMU, encoders, camera)
\Require $\mathcal{D}$ (demonstrations for Work;\;
  $\varnothing$ for Stand/Walk)
\Require $\eta$ (learning rate),\;
  $\gamma$ (dissipation),\;
  $\beta$ (soft-min sharpness),\;
  $L$ (depth),\; $n$ (width),\;
  $H$ (horizon),\;
  $N$ (eval count)
\Statex
\State $G,\; c_{\max},\; \theta_0,\; h
  \leftarrow \texttt{parse}(\mathrm{XML})$
  \Comment{graph, limits, pose, height}
\State $\{W_\ell, b_\ell\}_{\ell=1}^{L}
  \leftarrow \texttt{init}(L,\, n)$
  \Comment{init $\pi$;\; $c(e_\ell) = \|W_\ell\|_F$}
\Statex
\For{\textcolor{sword}{\textbf{phase}} $\in$
  \{Stand\,$(U\!=\!\{0\})$,\;\,
   Walk\,$(U\!=\!\mathrm{Unif})$,\;\,
   Work\,$(U\!=\!\mathrm{task})$\}}
  \Comment{\textcolor{sword}{$\varphi$: curriculum}}
  \Repeat
    \State $u \sim U$;\;\;
      $\{\tau_t^*\} \leftarrow \mathcal{D}(u)$
      \Comment{command $+$ demo ($\varnothing$ if Stand/Walk)}
    \For{$t = 0, \ldots, H$}
      \Comment{\textcolor{water}{水: rollout}}
      \State $o_t \leftarrow (o_{\mathrm{prop}},\,
        \varphi_{\mathrm{vis}}(I_t))$
        \Comment{\cref{def:observation}: $\R^{42+d}$}
      \State $\tau_t \leftarrow \textsc{Forward}(o_t,\, u)$
        \Comment{\cref{alg:forward}: $\R^{42+d} \to \R^{12}$}
      \State $q_{t+1} \leftarrow \mathcal{E}.\texttt{step}(q_t,\, \tau_t)$
        \Comment{EL dynamics}
    \EndFor
    \State \textcolor{water}{\textsc{Evaluate}:}\;
      $|\phi|_\beta \leftarrow
      \mathrm{soft\text{-}min}_\beta\bigl\{
      h(q_t)\!-\!h_{\min},\;\,
      \phi_{\max}\!-\!\|R_t\!-\!I\|_F
      \bigr\}_{t=0}^{H}$
      \Comment{\textcolor{water}{水: margin} (\cref{def:soft-viability})}
    \State $|\phi|_\beta \leftarrow
      \mathrm{soft\text{-}min}_\beta\bigl(
      |\phi|_\beta,\;\,
      \varepsilon\!-\!\|\tau_t\!-\!\tau_t^*\|
      \bigr)_{t}$
      \Comment{imitation (\cref{def:task-rkhs});
      skip if $\mathcal{D}\!=\!\varnothing$}
    \State \textcolor{water}{\textsc{Pullback}:}\;
      $\delta\tau \leftarrow
      \mathcal{E}.\texttt{grad}(|\phi|_\beta,\, \{\tau_t\})$
      \Comment{\textcolor{water}{水: engine autodiff}}
    \State \textcolor{water}{\textsc{Backward}:}\;
      $\{\nabla W,\, \nabla b\} \leftarrow
      \textsc{Backward}(\delta\tau)$
      \Comment{\cref{alg:backward}: 水$\leftarrow$ to $\pi$}
    \State $W_\ell \leftarrow W_\ell
      + \eta\bigl[\textcolor{water}{\nabla W_\ell}
      - \textcolor{knife}{\gamma\, W_\ell}\bigr]$;\;\;
      $b_\ell \leftarrow b_\ell
      + \eta\,\textcolor{water}{\nabla b_\ell}$
      \Comment{Eq.~\eqref{eq:contactflow} on $\pi$}
    \State \textcolor{knife}{\textsc{Clamp}:}\;
      $\|W_\ell\|_F \leftarrow
      \min\bigl(\|W_\ell\|_F,\;
      c_{\max}(e_\ell)\bigr)$
      \Comment{\textcolor{knife}{刀: enforce capacity}}
    \If{$|\phi|_\beta \le 0$}
      \Comment{robot fell}
      \State \textcolor{sword}{\textsc{Reset}:}\;
        $\mathcal{E}.\texttt{reset}(\theta_0)$;\;\;
        $\{W, b\} \leftarrow \texttt{init}(L, n)$
        \Comment{\textcolor{sword}{$\varphi$: recover}}
    \EndIf
  \Until{$\max|\phi| = \min|C|$ for $N$ consecutive rollouts}
\EndFor
\Statex
\Ensure Trained weights $\{W_\ell, b_\ell\}$:
  $\pi = \textsc{Forward}(\cdot;\, W, b)
  \colon \R^{42+d} \to \R^{12}$
\end{algorithmic}
\end{algorithm}

\begin{remark}[Validation chain: MuJoCo $+$ \texttt{go2\_mjx.xml}
$\Rightarrow$ trained policy]
\label{rem:validation-chain}
Every input to \cref{alg:loco} is extracted from two artifacts:
a physics engine (MuJoCo) and a robot model file
(\texttt{go2\_mjx.xml}, MuJoCo Menagerie).
No additional assumptions.
\begin{center}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Algorithm input} & \textbf{Source} & \textbf{Extraction} \\
\midrule
Graph $G$ (12 edges, 4 terminals)
  & XML & Joint topology (\texttt{<body>}/\texttt{<joint>}) \\
Capacity bounds $c_{\max}(e)$
  & XML & \texttt{<joint range>}, \texttt{<actuator ctrlrange>} \\
Standing pose $\theta_0$
  & XML & \texttt{<keyframe>} (default configuration) \\
Standing height $h$
  & XML & CoM of default keyframe ($0.312$\,m) \\
Proprioception $o_{\mathrm{prop}} \in \R^{42}$
  & Sensors & IMU ($R, \omega, v, p$) + encoders ($\theta, \dot\theta$) \\
Vision $o_{\mathrm{vis}} \in \R^{d}$
  & Camera & DINOv2 (frozen ViT, $d = 384$) \\
Command field $U$ (\cref{def:command})
  & User & Curriculum: $\delta_0$ / Uniform / task \\
EL dynamics $M, C, g$
  & Engine & \texttt{mj\_step}: $(q, \dot{q}, \tau) \mapsto \ddot{q}$ \\
Contact mode $c \in \{0,1\}^4$
  & Engine & \texttt{mj\_contact}: foot--ground detection \\
Gradient $\partial|\phi|/\partial c(e)$
  & Engine & MJX autodiff (JAX backend) \\
Reset to standing
  & Engine & \texttt{mj\_resetData} \\
\bottomrule
\end{tabular}
\end{center}
No reward shaping. No domain knowledge beyond what the XML file
contains. The viability axiom (\cref{ax:viability}) is the only
objective: $|\phi| > 0$ (do not fall). The knife
(\cref{def:dissipation}) is the only regulariser:
$\gamma(e) \cdot c(e)$ (do not exceed limits).

Given a MuJoCo engine and a \texttt{.xml} model file, every step of
\cref{alg:loco} is mechanically executable.
The standing structure (\cref{rem:standing}) is read from the file.
The contact dynamics (\cref{sec:contact}) is computed by the engine.
The algorithm is the bridge.
\end{remark}

\begin{remark}[The policy as parallel transport]\label{rem:gauge}
In \cref{alg:loco}, the policy
$\pi \colon \R^{42+d} \to \R^{12}$
maps sensor readings to joint torques (\cref{def:observation}).
Unfolding $\pi$ as a neural network (\cref{def:neural-exgraph}):
\[
  o \;\xrightarrow{\;\textcolor{water}{\sigma_1 \circ W_1}\;}
  h_1 \;\xrightarrow{\;\textcolor{water}{\sigma_2 \circ W_2}\;}
  \cdots \;\xrightarrow{\;\textcolor{water}{\sigma_L \circ W_L}\;}
  \tau.
\]
Each arrow is a \textsc{Slide} (\S\ref{sec:calculus}, four operations):
the $(42\!+\!d)$-dimensional observation flows through $L$ layers,
each applying $\textcolor{water}{\sigma \circ W}$.
The composite is \textsc{Compose}: the forward pass.

This is parallel transport on the execution graph.
The weights $\{W_\ell\}_{\ell=1}^L$ define a \emph{connection}:
a rule for transporting observations through the network.
Different weights $=$ different connection.
Training (the \textcolor{water}{水} steps of \cref{alg:loco})
searches for the connection under which the transported
observation produces viable torques: $|\phi| > 0$.

The trained network is a fixed connection on the standing structure.
The spectral gap $\lambda_1 > 0$ (\cref{thm:massgap}) is its
stability: small perturbations in observation decay, not amplify.
\end{remark}

\begin{remark}[Frozen and learned connections]\label{rem:frozen-gauge}
The observation $o = (o_{\mathrm{prop}},\, o_{\mathrm{vis}})$
(\cref{def:observation}) passes through \emph{two} connections in
series:
\[
  \underbrace{I_{\mathrm{cam}}
  \;\xrightarrow{\;\varphi_{\mathrm{vis}}\;}
  o_{\mathrm{vis}}}_
  {\text{frozen (DINOv2)}}
  \;\oplus\;
  o_{\mathrm{prop}}
  \;\xrightarrow{\;\pi\;}
  \tau.
\]
The visual encoder $\varphi_{\mathrm{vis}}$ is a \emph{frozen
connection}: a pretrained ViT whose weights are fixed during
\cref{alg:loco}. It transports raw pixels into a semantic
embedding space. The policy $\pi$ is a \emph{learned connection}:
its weights are updated by the \textcolor{water}{水} steps.

In gauge-theoretic language: $\varphi_{\mathrm{vis}}$ is a
background gauge field (fixed geometry of the visual fibre bundle),
while $\pi$ is the dynamical gauge field (trained by the contact
gradient flow~\eqref{eq:contactflow}).
The visual encoder sees the terrain; the policy decides what to do
about it. Two connections, one frozen, one learned.
\end{remark}

\begin{remark}[Deployment: real-time frequency alignment]
\label{rem:deployment}
\Cref{alg:loco} trains in simulated time
($\Delta t = 0.02$\,s, control rate $50$\,Hz).
Deployment on real hardware requires the same loop at the same rate
in \emph{real} time:
\[
  \underbrace{o_t \leftarrow \texttt{sense}}_
  {\text{read sensors}}
  \;\to\;
  \underbrace{\tau_t \leftarrow \pi(o_t)}_
  {\text{evaluate NN}}
  \;\to\;
  \underbrace{q_{t+1} \leftarrow \texttt{actuate}(\tau_t)}_
  {\text{command joints}}
  \;\leq\; 20\,\text{ms}.
\]
The entire sense--compute--act cycle must complete within one
$\Delta t$. If the policy $\pi$ (the connection of
\cref{rem:gauge}) takes longer than $20$\,ms to evaluate, the
real-time constraint is violated: the flow is no longer at the
trained frequency, and viability is not guaranteed.

This is a viability condition on the computation itself.
The standing structure (\cref{rem:standing}) specifies the physical
constraints; the frequency constraint specifies the computational
one. Both must hold for deployment:
\[
  \underbrace{|\phi| > 0}_{\text{physics: do not fall}}
  \quad\wedge\quad
  \underbrace{t_{\pi} \leq \Delta t}_
  {\text{compute: do not lag}}.
\]
Time must be real and linear.
No simulation speedup, no frame drops.
\end{remark}

\begin{remark}[Annealing $\beta$ as renormalisation group flow]
\label{rem:rg-annealing}
The soft-min parameter $\beta$ (\cref{def:soft-viability}) anneals
with the curriculum (\cref{def:command}):
\[
  \text{Stand} \;(\beta \text{ small})
  \;\longrightarrow\;
  \text{Walk} \;(\beta \text{ medium})
  \;\longrightarrow\;
  \text{Work} \;(\beta \text{ large}).
\]
This is a renormalisation group (RG) flow from the ultraviolet
(UV, smooth, all-timestep gradient) to the infrared (IR, sharp,
worst-timestep gradient).
At small $\beta$, the viability margin is a soft average: the
gradient is dense and the optimisation landscape is smooth---the
robot learns to stand by distributing information across the
entire trajectory.
At large $\beta$, the margin approaches the hard $\min$: the
gradient concentrates on the single worst timestep---the robot
learns precise footwork by focusing on the critical instant.

The \textsc{Phase} operator ($\textcolor{sword}{\varphi}$) of the
calculus drives this flow.
Each phase transition $U_k \to U_{k+1}$ increases both the command
support and the sharpness $\beta$.
The contact gradient flow~\eqref{eq:contactflow} operates at the
current $\beta$; the curriculum selects the scale.
UV $\to$ IR: smooth $\to$ sharp, Stand $\to$ Work,
all-timestep $\to$ worst-timestep.
\end{remark}

\begin{definition}[Task RKHS (学堂)]\label{def:task-rkhs}
Let $\mathcal{T}$ be a set of tasks, each observed via a camera
image $I_\tau$.
The \emph{task kernel} is
\begin{equation}\label{eq:task-kernel}
  k(\tau_1, \tau_2)
  \;=\;
  \bigl\langle
  \varphi_{\mathrm{vis}}(I_{\tau_1}),\;\,
  \varphi_{\mathrm{vis}}(I_{\tau_2})
  \bigr\rangle_{\R^d},
\end{equation}
the inner product of DINOv2 embeddings
(\cref{def:observation}, $d = 384$).
The \emph{task RKHS} $\mathcal{H}_k$ is the reproducing kernel
Hilbert space induced by $k$: the space of skill functions
$f \colon \mathcal{T} \to \R^{12}$ with reproducing property
$f(\tau) = \langle f,\, k(\cdot, \tau) \rangle_{\mathcal{H}_k}$.

A \emph{demonstration} for task $\tau$ is a trajectory
$\mathcal{D}_\tau = \{(o_t,\, \tau_t^*)\}_{t=0}^{H}$
of observation--torque pairs, collected from a teacher
(teleoperation, motion capture, or a reference policy).
The \emph{task margin} at timestep $t$ is
\begin{equation}\label{eq:task-margin}
  m_{\mathrm{task}}(t)
  \;=\;
  \varepsilon_{\mathrm{task}}
  \;-\;
  \bigl\|\pi(o_t, u) - \tau_t^*\bigr\|,
\end{equation}
where $\varepsilon_{\mathrm{task}} > 0$ is the imitation
tolerance.
The Work-phase viability margin
extends~\eqref{eq:soft-viability}:
\begin{equation}\label{eq:work-margin}
  |\phi|_\beta^{\mathrm{Work}}
  \;=\;
  \mathrm{soft\text{-}min}_\beta\bigl(
  \underbrace{h(q_t) - h_{\min}}_{\text{height}},\;\;
  \underbrace{\phi_{\max} - \|R_t - I\|_F}_
  {\text{orientation}},\;\;
  \underbrace{m_{\mathrm{task}}(t)}_
  {\text{imitation}}
  \bigr)_{t=0}^{H}.
\end{equation}
During Stand and Walk, $\mathcal{D}_\tau = \varnothing$ and the
task term is absent ($e^{-\beta \cdot \infty} = 0$ in the
soft-min).
During Work, the demonstration is the teacher, and the kernel
$k$ provides generalisation: a policy trained on demonstrated
tasks $\{\tau_i\}$ transfers to a new task $\tau^*$ in proportion
to $k(\tau^*, \tau_i)$.
\end{definition}

\begin{remark}[The frozen eye as task metric]
\label{rem:task-metric}
The DINOv2 encoder $\varphi_{\mathrm{vis}}$
(\cref{def:observation}) serves two roles:
\begin{enumerate}[label=(\roman*)]
  \item \textbf{Observation}: maps camera images to
  $o_{\mathrm{vis}} \in \R^d$ for the policy
  (\cref{alg:forward}).
  \item \textbf{Task metric}: the kernel
  $k$~\eqref{eq:task-kernel} defines the geometry of the task
  space $\mathcal{T}$ (\cref{def:task-rkhs}).
\end{enumerate}
One encoder, two roles.
The differentiation boundary (\cref{rem:frozen-gauge}) is also
the metric boundary: the fixed geometry on which all tasks are
measured.

Because DINOv2 is pretrained on real images (ImageNet), the
kernel $k$ is anchored in reality, not simulation.
Tasks that look similar in the real world are close in
$\mathcal{H}_k$.
This bridges the sim-to-real gap for the Work phase:
a policy trained on task $\tau_1$ in simulation transfers to
a visually similar task $\tau_2$ in reality because
$k(\tau_1, \tau_2)$ is large.
The school (学堂) generalises through the kernel.
Demonstrations are lessons; the RKHS norm is the grade;
deployment is graduation.
\end{remark}

\begin{remark}[Hyperparameter atlas]\label{rem:hyperparameters}
\Cref{alg:loco} takes seven scalar hyperparameters.
They split into two tiers by origin.

\medskip
\noindent
\textbf{Tier 1: physics-determined}
(read from the engine or the task; not free choices).

\smallskip
{\small
\begin{center}
\begin{tabular}{@{}l l p{0.62\textwidth}@{}}
\toprule
Symbol & Source & Interpretation \\
\midrule
$\gamma$ &
  \texttt{XML} damping &
  Dissipation rate (thermodynamic 2nd law of the engine).
  Read from \texttt{go2\_mjx.xml}. \\
$H$ &
  Task duration &
  Horizon: timesteps per rollout.
  Typical: $500$--$2000$ (at $\Delta t$ of XML). \\
$\eta$ &
  Optimiser &
  Learning rate: the Planck scale of the update---the smallest
  step that moves $W$ meaningfully.
  $\eta \ll \gamma^{-1}$ ensures
  the flow~\eqref{eq:contactflow} is contractive.
  Typical: $10^{-4}$--$10^{-3}$. \\
\bottomrule
\end{tabular}
\end{center}
}

\smallskip
$\gamma$ is not a free knob: it is the physical dissipation
already baked into the simulator's contact model.
$H$ is set by the task (how long one episode lasts).
$\eta$ is fundamental: it converts the gradient $\nabla W$ into a
finite displacement, analogous to the Planck time converting
energy into frequency ($E = \hbar\omega$).
Too large $\Rightarrow$ instability; too small $\Rightarrow$
the agent never moves.

\medskip
\noindent
\textbf{Tier 2: user settings}
(design choices; tune on validation rollouts).

\smallskip
{\small
\begin{center}
\begin{tabular}{@{}l l p{0.62\textwidth}@{}}
\toprule
Symbol & Role & Interpretation \\
\midrule
$L$ &
  Depth &
  Number of layers in $\pi$ (\cref{alg:forward}).
  Typical: $3$--$5$. \\
$n$ &
  Width &
  Hidden dimension per layer.
  Typical: $128$--$512$. \\
$N$ &
  Eval count &
  Rollouts per gradient step (sample size for
  $|\phi|_\beta$).
  Typical: $64$--$4096$. \\
$\beta$ &
  Sharpness &
  RG scale (\cref{rem:rg-annealing}): small $\beta$ $=$ UV
  (smooth, exploratory); large $\beta$ $=$ IR (sharp,
  exploitative).
  Typical: $1$--$100$. \\
$\varepsilon_{\mathrm{task}}$ &
  Imitation &
  Task margin (\cref{def:task-rkhs});
  $\varnothing$ during Stand/Walk.
  Task-dependent. \\
\bottomrule
\end{tabular}
\end{center}
}

\smallskip
The tier boundary is sharp: Tier~1 parameters are
\emph{measured} (from the XML, the task, or the optimiser's
stability condition); Tier~2 parameters are
\emph{chosen} (by the user, validated by rollout performance).
A practitioner who changes the robot changes Tier~1;
a practitioner who changes the architecture changes Tier~2.
Neither tier crosses into the other.
\end{remark}

\begin{remark}[\textcolor{caution}{No tricks needed}]
\label{rem:no-tricks}
\Cref{alg:loco} is gradient descent through a differentiable
simulator.  That is all.  We collect no replay buffer, fit no
value function, clip no surrogate objective, aggregate no
datasets.  Specifically:

\begin{itemize}[leftmargin=*]
\item \textcolor{caution}{\textbf{DAgger}} (Dataset Aggregation)
  fixes distribution shift: the policy at training time sees
  states it would not visit at test time.
  Here there is no shift---every rollout in \cref{alg:loco}
  starts from $\theta_0$ and runs the \emph{current} $\pi$
  through the engine.
  The training distribution \emph{is} the deployment
  distribution at every iteration.
  No aggregation needed.

\item \textcolor{caution}{\textbf{PPO}} (Proximal Policy
  Optimisation) is a sum of heuristics.
  Strip them one by one:
  \begin{enumerate}[label=(\roman*)]
    \item \emph{Surrogate objective} $\to$ unnecessary:
      MJX gives $\nabla_\tau |\phi|_\beta$ exactly
      (\textsc{Pullback}, \cref{alg:loco}).
      No need to approximate the gradient via a clipped ratio.
    \item \emph{Value baseline} $\to$ unnecessary:
      $|\phi|_\beta$ is computed per-rollout, not estimated
      by a learned $V(s)$.
    \item \emph{Clipping} $\to$ unnecessary:
      exact gradients do not explode;
      the capacity clamp (\textsc{Clamp}, \cref{alg:loco})
      bounds $\|W_\ell\|_F$ directly.
    \item \emph{Entropy bonus} $\to$ unnecessary:
      command sampling $u \sim U$ provides exploration;
      $\beta$-annealing (\cref{rem:rg-annealing}) controls
      the sharpness schedule.
  \end{enumerate}
  What remains after stripping?
  Gradient descent with dissipation ($\gamma\, W$).
  That is \cref{alg:loco}.

\item \textcolor{caution}{\textbf{REINFORCE}} / score-function
  estimators: high variance because they differentiate through
  $\log \pi$ instead of through the dynamics.
  MJX differentiates through the dynamics.
  Variance $\to$ zero (up to finite $N$).
\end{itemize}

The pattern: every ``trick'' in model-free RL is a patch for a
missing gradient.
A differentiable engine provides the gradient.
The patches become unnecessary.
\Cref{alg:loco} is not a new algorithm---it is what remains when
you \textcolor{caution}{delete the patches}.
\end{remark}
